{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4466617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "pdir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(pdir)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import utils\n",
    "from simpleview_pytorch import SimpleView\n",
    "\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f57322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmja2106\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb7578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fbc0d",
   "metadata": {},
   "source": [
    "### Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805c6798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUEFAG     1116\n",
      "PINNIG      581\n",
      "QUEILE      364\n",
      "PINSYL      277\n",
      "PINPIN      140\n",
      "JUNIPE        2\n",
      "NA            2\n",
      "QUERCUS       2\n",
      "DEAD          1\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['DEAD', 'JUNIPE', 'NA', 'PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE', 'QUERCUS']\n",
      "Labels:  tensor([8, 3, 6,  ..., 7, 3, 6])\n",
      "Total count:  2485\n"
     ]
    }
   ],
   "source": [
    "trees_data = torch.load('trees_128.pt')\n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e012318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.28 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">expert-donkey-12</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mja2106/laser-trees\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mja2106/laser-trees/runs/90nz2tpx\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees/runs/90nz2tpx</a><br/>\n",
       "                Run data is saved locally in <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210501_152438-90nz2tpx</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"batch_size\":128,\n",
    "    \"validation_split\":.2,\n",
    "    \"shuffle_dataset\":True,\n",
    "    \"random_seed\":0,\n",
    "    \"learning_rate\":0.0005,\n",
    "    \"momentum\":0.9,\n",
    "    \"epochs\":100,\n",
    "    \"loss_fn\":\"cross-entropy\",\n",
    "    \"optimizer\":\"sgd\",\n",
    "    \"jitter\":False,\n",
    "    \"random_rotation\":False,\n",
    "    \"random_scaling\":False,\n",
    "    \"random_translation\":False,\n",
    "    \"voting\":\"None\",\n",
    "    \n",
    "    \"model\":\"SimpleView\",\n",
    "    \n",
    "    \"image_dim\":trees_data.image_dim,\n",
    "    \"camera_fov_deg\":trees_data.camera_fov_deg,\n",
    "    \"f\":trees_data.f,\n",
    "    \"camera_dist\":trees_data.camera_dist,\n",
    "    \"num_views\":trees_data.depth_images.shape[1],\n",
    "    \n",
    "    \"species\":[\"QUEFAG\", \"PINNIG\", \"QUEILE\", \"PINSYL\", \"PINPIN\"]\n",
    "}\n",
    "\n",
    "experiment_name = wandb.util.generate_id()\n",
    "    \n",
    "run = wandb.init(\n",
    "    project='laser-trees',\n",
    "    group=experiment_name,\n",
    "    config=params)    \n",
    "\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea239dbf",
   "metadata": {},
   "source": [
    "#### Remove low-count species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca3f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing: DEAD\n",
      "Removing: JUNIPE\n",
      "Removing: QUERCUS\n",
      "Removing: NA\n",
      "QUEFAG    1116\n",
      "PINNIG     581\n",
      "QUEILE     364\n",
      "PINSYL     277\n",
      "PINPIN     140\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE']\n",
      "Labels:  tensor([0, 3, 3,  ..., 4, 0, 3])\n",
      "Total count:  2478\n"
     ]
    }
   ],
   "source": [
    "for specie in list(set(trees_data.species) - set(config.species)):\n",
    "    print(\"Removing: {}\".format(specie))\n",
    "    trees_data.remove_species(specie)\n",
    "    \n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fb55e",
   "metadata": {},
   "source": [
    "#### Train-validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce2d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(trees_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(config.validation_split * dataset_size))\n",
    "\n",
    "if config.shuffle_dataset :\n",
    "    np.random.seed(config.random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7607fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22ab070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to print no. instances of each class\n",
    "if None:\n",
    "    for x in train_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()\n",
    "\n",
    "    for x in validation_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b49603",
   "metadata": {},
   "source": [
    "### Define model, loss fn, optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "798df85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(config.species) == set(trees_data.species)\n",
    "\n",
    "if config.model==\"SimpleView\":\n",
    "    model = SimpleView(\n",
    "        num_views=config.num_views,\n",
    "        num_classes=len(config.species)\n",
    "    )\n",
    "\n",
    "model = model.to(device=device)\n",
    "\n",
    "if config.loss_fn==\"cross-entropy\":\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "if config.optimizer==\"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=config.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7103990",
   "metadata": {},
   "source": [
    "### Train & Test Loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071e7b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/anaconda3/envs/laser-trees/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 4.000\n",
      "[1,    10] loss: 3.441\n",
      "[1,    15] loss: 3.589\n",
      "Got 231 / 495 with accuracy 46.67\n",
      "[2,     5] loss: 3.305\n",
      "[2,    10] loss: 3.317\n",
      "[2,    15] loss: 3.397\n",
      "Got 129 / 495 with accuracy 26.06\n",
      "[3,     5] loss: 3.256\n",
      "[3,    10] loss: 3.249\n",
      "[3,    15] loss: 3.101\n",
      "Got 137 / 495 with accuracy 27.68\n",
      "[4,     5] loss: 3.159\n",
      "[4,    10] loss: 3.034\n",
      "[4,    15] loss: 3.038\n",
      "Got 290 / 495 with accuracy 58.59\n",
      "[5,     5] loss: 2.986\n",
      "[5,    10] loss: 3.016\n",
      "[5,    15] loss: 2.945\n",
      "Got 293 / 495 with accuracy 59.19\n",
      "[6,     5] loss: 2.936\n",
      "[6,    10] loss: 2.914\n",
      "[6,    15] loss: 2.877\n",
      "Got 290 / 495 with accuracy 58.59\n",
      "[7,     5] loss: 2.896\n",
      "[7,    10] loss: 2.768\n",
      "[7,    15] loss: 2.892\n",
      "Got 299 / 495 with accuracy 60.40\n",
      "[8,     5] loss: 2.903\n",
      "[8,    10] loss: 2.776\n",
      "[8,    15] loss: 2.709\n",
      "Got 293 / 495 with accuracy 59.19\n",
      "[9,     5] loss: 2.733\n",
      "[9,    10] loss: 2.656\n",
      "[9,    15] loss: 2.861\n",
      "Got 304 / 495 with accuracy 61.41\n",
      "[10,     5] loss: 2.671\n",
      "[10,    10] loss: 2.778\n",
      "[10,    15] loss: 2.712\n",
      "Got 300 / 495 with accuracy 60.61\n",
      "[11,     5] loss: 2.691\n",
      "[11,    10] loss: 2.725\n",
      "[11,    15] loss: 2.611\n",
      "Got 303 / 495 with accuracy 61.21\n",
      "[12,     5] loss: 2.657\n",
      "[12,    10] loss: 2.620\n",
      "[12,    15] loss: 2.652\n",
      "Got 307 / 495 with accuracy 62.02\n",
      "[13,     5] loss: 2.718\n",
      "[13,    10] loss: 2.477\n",
      "[13,    15] loss: 2.646\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[14,     5] loss: 2.548\n",
      "[14,    10] loss: 2.588\n",
      "[14,    15] loss: 2.612\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[15,     5] loss: 2.608\n",
      "[15,    10] loss: 2.626\n",
      "[15,    15] loss: 2.442\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[16,     5] loss: 2.559\n",
      "[16,    10] loss: 2.571\n",
      "[16,    15] loss: 2.410\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[17,     5] loss: 2.525\n",
      "[17,    10] loss: 2.423\n",
      "[17,    15] loss: 2.606\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[18,     5] loss: 2.510\n",
      "[18,    10] loss: 2.430\n",
      "[18,    15] loss: 2.482\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[19,     5] loss: 2.425\n",
      "[19,    10] loss: 2.546\n",
      "[19,    15] loss: 2.380\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[20,     5] loss: 2.299\n",
      "[20,    10] loss: 2.621\n",
      "[20,    15] loss: 2.368\n",
      "Got 320 / 495 with accuracy 64.65\n",
      "[21,     5] loss: 2.404\n",
      "[21,    10] loss: 2.347\n",
      "[21,    15] loss: 2.430\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[22,     5] loss: 2.348\n",
      "[22,    10] loss: 2.317\n",
      "[22,    15] loss: 2.447\n",
      "Got 320 / 495 with accuracy 64.65\n",
      "[23,     5] loss: 2.338\n",
      "[23,    10] loss: 2.392\n",
      "[23,    15] loss: 2.316\n",
      "Got 321 / 495 with accuracy 64.85\n",
      "[24,     5] loss: 2.259\n",
      "[24,    10] loss: 2.451\n",
      "[24,    15] loss: 2.328\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[25,     5] loss: 2.421\n",
      "[25,    10] loss: 2.346\n",
      "[25,    15] loss: 2.135\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[26,     5] loss: 2.311\n",
      "[26,    10] loss: 2.374\n",
      "[26,    15] loss: 2.183\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[27,     5] loss: 2.358\n",
      "[27,    10] loss: 2.306\n",
      "[27,    15] loss: 2.158\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[28,     5] loss: 2.300\n",
      "[28,    10] loss: 2.318\n",
      "[28,    15] loss: 2.124\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[29,     5] loss: 2.145\n",
      "[29,    10] loss: 2.218\n",
      "[29,    15] loss: 2.289\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[30,     5] loss: 2.169\n",
      "[30,    10] loss: 2.216\n",
      "[30,    15] loss: 2.263\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[31,     5] loss: 2.155\n",
      "[31,    10] loss: 2.233\n",
      "[31,    15] loss: 2.139\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[32,     5] loss: 2.116\n",
      "[32,    10] loss: 2.162\n",
      "[32,    15] loss: 2.165\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[33,     5] loss: 2.108\n",
      "[33,    10] loss: 2.121\n",
      "[33,    15] loss: 2.210\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[34,     5] loss: 2.061\n",
      "[34,    10] loss: 2.121\n",
      "[34,    15] loss: 2.140\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[35,     5] loss: 2.007\n",
      "[35,    10] loss: 2.210\n",
      "[35,    15] loss: 2.008\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[36,     5] loss: 1.996\n",
      "[36,    10] loss: 2.109\n",
      "[36,    15] loss: 2.098\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[37,     5] loss: 2.031\n",
      "[37,    10] loss: 2.096\n",
      "[37,    15] loss: 2.001\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[38,     5] loss: 2.016\n",
      "[38,    10] loss: 2.119\n",
      "[38,    15] loss: 1.967\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[39,     5] loss: 1.940\n",
      "[39,    10] loss: 2.064\n",
      "[39,    15] loss: 1.988\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[40,     5] loss: 1.950\n",
      "[40,    10] loss: 2.046\n",
      "[40,    15] loss: 1.944\n",
      "Got 320 / 495 with accuracy 64.65\n",
      "[41,     5] loss: 2.017\n",
      "[41,    10] loss: 1.977\n",
      "[41,    15] loss: 1.934\n",
      "Got 315 / 495 with accuracy 63.64\n",
      "[42,     5] loss: 2.011\n",
      "[42,    10] loss: 1.771\n",
      "[42,    15] loss: 2.036\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[43,     5] loss: 1.801\n",
      "[43,    10] loss: 1.911\n",
      "[43,    15] loss: 1.945\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[44,     5] loss: 1.927\n",
      "[44,    10] loss: 1.851\n",
      "[44,    15] loss: 1.888\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[45,     5] loss: 1.853\n",
      "[45,    10] loss: 1.822\n",
      "[45,    15] loss: 1.944\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[46,     5] loss: 1.828\n",
      "[46,    10] loss: 1.917\n",
      "[46,    15] loss: 1.771\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[47,     5] loss: 1.877\n",
      "[47,    10] loss: 1.858\n",
      "[47,    15] loss: 1.708\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[48,     5] loss: 1.814\n",
      "[48,    10] loss: 1.837\n",
      "[48,    15] loss: 1.802\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[49,     5] loss: 1.726\n",
      "[49,    10] loss: 1.802\n",
      "[49,    15] loss: 1.756\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[50,     5] loss: 1.706\n",
      "[50,    10] loss: 1.755\n",
      "[50,    15] loss: 1.758\n",
      "Got 314 / 495 with accuracy 63.43\n",
      "[51,     5] loss: 1.713\n",
      "[51,    10] loss: 1.753\n",
      "[51,    15] loss: 1.637\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[52,     5] loss: 1.676\n",
      "[52,    10] loss: 1.658\n",
      "[52,    15] loss: 1.725\n",
      "Got 314 / 495 with accuracy 63.43\n",
      "[53,     5] loss: 1.718\n",
      "[53,    10] loss: 1.693\n",
      "[53,    15] loss: 1.649\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[54,     5] loss: 1.561\n",
      "[54,    10] loss: 1.596\n",
      "[54,    15] loss: 1.717\n",
      "Got 315 / 495 with accuracy 63.64\n",
      "[55,     5] loss: 1.612\n",
      "[55,    10] loss: 1.640\n",
      "[55,    15] loss: 1.577\n",
      "Got 315 / 495 with accuracy 63.64\n",
      "[56,     5] loss: 1.586\n",
      "[56,    10] loss: 1.659\n",
      "[56,    15] loss: 1.579\n",
      "Got 310 / 495 with accuracy 62.63\n",
      "[57,     5] loss: 1.486\n",
      "[57,    10] loss: 1.637\n",
      "[57,    15] loss: 1.556\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[58,     5] loss: 1.505\n",
      "[58,    10] loss: 1.632\n",
      "[58,    15] loss: 1.499\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[59,     5] loss: 1.509\n",
      "[59,    10] loss: 1.502\n",
      "[59,    15] loss: 1.520\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[60,     5] loss: 1.467\n",
      "[60,    10] loss: 1.466\n",
      "[60,    15] loss: 1.516\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[61,     5] loss: 1.475\n",
      "[61,    10] loss: 1.364\n",
      "[61,    15] loss: 1.488\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[62,     5] loss: 1.425\n",
      "[62,    10] loss: 1.495\n",
      "[62,    15] loss: 1.379\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[63,     5] loss: 1.384\n",
      "[63,    10] loss: 1.430\n",
      "[63,    15] loss: 1.395\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[64,     5] loss: 1.496\n",
      "[64,    10] loss: 1.306\n",
      "[64,    15] loss: 1.379\n",
      "Got 314 / 495 with accuracy 63.43\n",
      "[65,     5] loss: 1.421\n",
      "[65,    10] loss: 1.356\n",
      "[65,    15] loss: 1.266\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[66,     5] loss: 1.279\n",
      "[66,    10] loss: 1.301\n",
      "[66,    15] loss: 1.395\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[67,     5] loss: 1.347\n",
      "[67,    10] loss: 1.291\n",
      "[67,    15] loss: 1.310\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[68,     5] loss: 1.320\n",
      "[68,    10] loss: 1.244\n",
      "[68,    15] loss: 1.237\n",
      "Got 304 / 495 with accuracy 61.41\n",
      "[69,     5] loss: 1.259\n",
      "[69,    10] loss: 1.281\n",
      "[69,    15] loss: 1.199\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[70,     5] loss: 1.258\n",
      "[70,    10] loss: 1.115\n",
      "[70,    15] loss: 1.252\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[71,     5] loss: 1.164\n",
      "[71,    10] loss: 1.139\n",
      "[71,    15] loss: 1.260\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[72,     5] loss: 1.093\n",
      "[72,    10] loss: 1.148\n",
      "[72,    15] loss: 1.194\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[73,     5] loss: 1.107\n",
      "[73,    10] loss: 1.153\n",
      "[73,    15] loss: 1.138\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[74,     5] loss: 1.112\n",
      "[74,    10] loss: 1.107\n",
      "[74,    15] loss: 1.128\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[75,     5] loss: 1.077\n",
      "[75,    10] loss: 1.091\n",
      "[75,    15] loss: 1.065\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[76,     5] loss: 1.046\n",
      "[76,    10] loss: 0.994\n",
      "[76,    15] loss: 1.156\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[77,     5] loss: 1.032\n",
      "[77,    10] loss: 0.993\n",
      "[77,    15] loss: 1.077\n",
      "Got 306 / 495 with accuracy 61.82\n",
      "[78,     5] loss: 1.041\n",
      "[78,    10] loss: 0.986\n",
      "[78,    15] loss: 1.004\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[79,     5] loss: 0.987\n",
      "[79,    10] loss: 0.979\n",
      "[79,    15] loss: 0.933\n",
      "Got 307 / 495 with accuracy 62.02\n",
      "[80,     5] loss: 0.924\n",
      "[80,    10] loss: 0.959\n",
      "[80,    15] loss: 0.978\n",
      "Got 310 / 495 with accuracy 62.63\n",
      "[81,     5] loss: 0.949\n",
      "[81,    10] loss: 0.951\n",
      "[81,    15] loss: 0.968\n",
      "Got 307 / 495 with accuracy 62.02\n",
      "[82,     5] loss: 0.905\n",
      "[82,    10] loss: 0.937\n",
      "[82,    15] loss: 0.932\n",
      "Got 303 / 495 with accuracy 61.21\n",
      "[83,     5] loss: 0.891\n",
      "[83,    10] loss: 0.937\n",
      "[83,    15] loss: 0.883\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[84,     5] loss: 0.879\n",
      "[84,    10] loss: 0.904\n",
      "[84,    15] loss: 0.854\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[85,     5] loss: 0.829\n",
      "[85,    10] loss: 0.875\n",
      "[85,    15] loss: 0.891\n",
      "Got 304 / 495 with accuracy 61.41\n",
      "[86,     5] loss: 0.866\n",
      "[86,    10] loss: 0.863\n",
      "[86,    15] loss: 0.825\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[87,     5] loss: 0.832\n",
      "[87,    10] loss: 0.788\n",
      "[87,    15] loss: 0.807\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[88,     5] loss: 0.807\n",
      "[88,    10] loss: 0.781\n",
      "[88,    15] loss: 0.756\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[89,     5] loss: 0.741\n",
      "[89,    10] loss: 0.771\n",
      "[89,    15] loss: 0.786\n",
      "Got 301 / 495 with accuracy 60.81\n",
      "[90,     5] loss: 0.734\n",
      "[90,    10] loss: 0.752\n",
      "[90,    15] loss: 0.745\n",
      "Got 307 / 495 with accuracy 62.02\n",
      "[91,     5] loss: 0.753\n",
      "[91,    10] loss: 0.681\n",
      "[91,    15] loss: 0.753\n",
      "Got 310 / 495 with accuracy 62.63\n",
      "[92,     5] loss: 0.677\n",
      "[92,    10] loss: 0.712\n",
      "[92,    15] loss: 0.744\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[93,     5] loss: 0.731\n",
      "[93,    10] loss: 0.644\n",
      "[93,    15] loss: 0.676\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[94,     5] loss: 0.692\n",
      "[94,    10] loss: 0.677\n",
      "[94,    15] loss: 0.692\n",
      "Got 304 / 495 with accuracy 61.41\n",
      "[95,     5] loss: 0.647\n",
      "[95,    10] loss: 0.628\n",
      "[95,    15] loss: 0.713\n",
      "Got 309 / 495 with accuracy 62.42\n",
      "[96,     5] loss: 0.608\n",
      "[96,    10] loss: 0.627\n",
      "[96,    15] loss: 0.673\n",
      "Got 303 / 495 with accuracy 61.21\n",
      "[97,     5] loss: 0.635\n",
      "[97,    10] loss: 0.606\n",
      "[97,    15] loss: 0.592\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[98,     5] loss: 0.596\n",
      "[98,    10] loss: 0.603\n",
      "[98,    15] loss: 0.593\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[99,     5] loss: 0.552\n",
      "[99,    10] loss: 0.578\n",
      "[99,    15] loss: 0.627\n",
      "Got 306 / 495 with accuracy 61.82\n",
      "[100,     5] loss: 0.559\n",
      "[100,    10] loss: 0.581\n",
      "[100,    15] loss: 0.563\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.watch(model)\n",
    "for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    #Training loop============================================\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        depth_images = data['depth_images']\n",
    "        labels = data['labels']\n",
    "\n",
    "        depth_images = depth_images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(depth_images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "                \n",
    "    #Test loop================================================\n",
    "    num_train_correct = 0\n",
    "    num_train_samples = 0\n",
    "    \n",
    "    num_val_correct = 0\n",
    "    num_val_samples = 0\n",
    "    \n",
    "    running_train_loss = 0\n",
    "    running_val_loss = 0\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        #Train set eval==============\n",
    "        for data in train_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_train_correct += (predictions == labels).sum()\n",
    "            num_train_samples += predictions.size(0)\n",
    "            \n",
    "            running_train_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        train_acc = float(num_train_correct)/float(num_train_samples)\n",
    "        train_loss = running_train_loss/len(validation_loader)\n",
    "        \n",
    "        #Test set eval===============\n",
    "        for data in validation_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_val_correct += (predictions == labels).sum()\n",
    "            num_val_samples += predictions.size(0)\n",
    "            \n",
    "            running_val_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        val_acc = float(num_val_correct)/float(num_val_samples)\n",
    "        val_loss = running_val_loss/len(validation_loader)\n",
    "        \n",
    "        print(f'Got {num_val_correct} / {num_val_samples} with accuracy {val_acc*100:.2f}')\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Train Loss\":train_loss,\n",
    "            \"Validation Loss\":val_loss,\n",
    "            \"Train Accuracy\":train_acc,\n",
    "            \"Validation Accuracy\":val_acc\n",
    "            })\n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646148f",
   "metadata": {},
   "source": [
    "### Update old object to new class (Don't run every time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7de262",
   "metadata": {},
   "outputs": [],
   "source": [
    "if None:\n",
    "    metadata_file = \"../data/treesXYZ/meta/META.csv\"\n",
    "    data_dir = \"../data/treesXYZ/\"\n",
    "    trees_new = utils.TreeSpeciesDataset(data_dir, metadata_file)\n",
    "\n",
    "    trees_old=torch.load('trees_old.pt')\n",
    "\n",
    "    trees_new.depth_images = trees_old.depth_images\n",
    "    trees_new.labels = trees_old.labels.long()\n",
    "\n",
    "    torch.save(trees_new, 'trees_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e274b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if None: \n",
    "    metadata_file = \"../data/treesXYZ/meta/META.csv\"\n",
    "    data_dir = \"../data/treesXYZ/\"\n",
    "    trees_tmp = utils.TreeSpeciesDataset(data_dir=data_dir, metadata_file=metadata_file)\n",
    "    trees_tmp.depth_images=trees_data.depth_images\n",
    "    trees_tmp.labels=trees_data.labels\n",
    "    trees_tmp.image_dim=128\n",
    "    trees_tmp.camera_fov_deg=90\n",
    "    trees_tmp.f=1\n",
    "    trees_tmp.camera_dist=1.4\n",
    "\n",
    "    torch.save(trees_tmp, 'trees_128.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
