{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ace2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "pdir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(pdir)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import utils\n",
    "from simpleview_pytorch import SimpleView\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "model_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5877f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmja2106\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2678d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660accf",
   "metadata": {},
   "source": [
    "### Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9531bde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUEFAG     1116\n",
      "PINNIG      581\n",
      "QUEILE      364\n",
      "PINSYL      277\n",
      "PINPIN      140\n",
      "NA            2\n",
      "JUNIPE        2\n",
      "QUERCUS       2\n",
      "DEAD          1\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['DEAD', 'JUNIPE', 'NA', 'PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE', 'QUERCUS']\n",
      "Labels:  tensor([8, 3, 6,  ..., 7, 3, 6])\n",
      "Total count:  2485\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tree_points.pt'\n",
    "\n",
    "trees_data = torch.load(dataset_name)\n",
    "val_data = torch.load(dataset_name)\n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2425dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3]) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18649c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">deep-durian-74</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mja2106/laser-trees\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mja2106/laser-trees/runs/1cp4079e\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees/runs/1cp4079e</a><br/>\n",
       "                Run data is saved locally in <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210519_085545-1cp4079e</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\n",
    "    \"dataset_type\":type(trees_data),\n",
    "    \"batch_size\":128,\n",
    "    \"validation_split\":.2,\n",
    "    \"shuffle_dataset\":True,\n",
    "    \"random_seed\":0,\n",
    "    \"learning_rate\":[0.0005, 100, 0.5],  #[init, step_size, gamma] for scheduler\n",
    "    \"momentum\":0.9, #Only used for sgd\n",
    "    \"epochs\":300,\n",
    "    \"loss_fn\":\"cross-entropy\",\n",
    "    \"optimizer\":\"adam\",\n",
    "    \"voting\":\"None\",\n",
    "    \n",
    "    \"model\":\"SimpleView\",\n",
    "    \n",
    "    \"image_dim\":trees_data.image_dim,\n",
    "    \"camera_fov_deg\":trees_data.camera_fov_deg,\n",
    "    \"f\":trees_data.f,\n",
    "    \"camera_dist\":trees_data.camera_dist,\n",
    "    \"depth_averaging\":\"min\",\n",
    "    \n",
    "    \"transforms\":['rotation','translation','jitter'],\n",
    "    \"min_rotation\":0,\n",
    "    \"max_rotation\":2*np.pi,\n",
    "    \"min_translation\":0,\n",
    "    \"max_translation\":0.5,\n",
    "    \"jitter_std\":3e-4, \n",
    "    \n",
    "    \"species\":[\"QUEFAG\", \"PINNIG\", \"QUEILE\", \"PINSYL\", \"PINPIN\"],\n",
    "    \"data_resolution\":\"2.5cm\"\n",
    "}\n",
    "\n",
    "\n",
    "if params[\"dataset_type\"] == utils.dataset.TreeSpeciesPointDataset: #Change these by hand using point dataset\n",
    "    params[\"image_dim\"] = 256\n",
    "    params[\"camera_fov_deg\"] = 90 \n",
    "    params[\"f\"] = 1\n",
    "    params[\"camera_dist\"] = 1.4\n",
    "    params[\"depth_averaging\"] = \"min\"\n",
    "    params[\"soft_min_k\"] = 50\n",
    "    params[\"num_views\"] = 6\n",
    "    \n",
    "    trees_data.set_params(image_dim = params[\"image_dim\"],\n",
    "                         camera_fov_deg = params[\"camera_fov_deg\"],\n",
    "                         f = params[\"f\"],\n",
    "                         camera_dist = params[\"camera_dist\"],\n",
    "                         soft_min_k = params[\"soft_min_k\"],\n",
    "                         transforms = params[\"transforms\"],\n",
    "                         min_rotation = params[\"min_rotation\"],\n",
    "                         max_rotation = params[\"max_rotation\"],\n",
    "                         min_translation = params[\"min_translation\"],\n",
    "                         max_translation = params[\"max_translation\"],\n",
    "                         jitter_std = params[\"jitter_std\"]\n",
    "                         )\n",
    "    \n",
    "    val_data.set_params(image_dim = params[\"image_dim\"],\n",
    "                         camera_fov_deg = params[\"camera_fov_deg\"],\n",
    "                         f = params[\"f\"],\n",
    "                         camera_dist = params[\"camera_dist\"],\n",
    "                         soft_min_k = params[\"soft_min_k\"],\n",
    "                         transforms = params[\"transforms\"],\n",
    "                         min_rotation = params[\"min_rotation\"],\n",
    "                         max_rotation = params[\"max_rotation\"],\n",
    "                         min_translation = params[\"min_translation\"],\n",
    "                         max_translation = params[\"max_translation\"],\n",
    "                         jitter_std = params[\"jitter_std\"]\n",
    "                         )\n",
    "    \n",
    "elif params[\"dataset_type\"] == utils.dataset.TreeSpeciesDataset:\n",
    "    params[\"image_dim\"] = trees_data.image_dim\n",
    "    params[\"camera_fov_deg\"] = trees_data.camera_fov_deg\n",
    "    params[\"f\"] = trees_data.f\n",
    "    params[\"camera_dist\"] = trees_data.camera_dist\n",
    "    params[\"num_views\"] = trees_data.depth_images.shape[1]\n",
    "    params[\"depth_averaging\"] = \"min\"\n",
    "\n",
    "    \n",
    "if trees_data.soft_min_k:\n",
    "    params[\"soft_min_k\"] = trees_data.soft_min_k\n",
    "\n",
    "experiment_name = wandb.util.generate_id()\n",
    "    \n",
    "run = wandb.init(\n",
    "    project='laser-trees',\n",
    "    group=experiment_name,\n",
    "    config=params)    \n",
    "\n",
    "config = wandb.config\n",
    "torch.manual_seed(config.random_seed)\n",
    "torch.cuda.manual_seed(config.random_seed)\n",
    "np.random.seed(config.random_seed)\n",
    "random.seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa52ae",
   "metadata": {},
   "source": [
    "#### Remove low-count species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "741cd064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing: QUERCUS\n",
      "Removing: DEAD\n",
      "Removing: NA\n",
      "Removing: JUNIPE\n",
      "Train Dataset:\n",
      "QUEFAG    1116\n",
      "PINNIG     581\n",
      "QUEILE     364\n",
      "PINSYL     277\n",
      "PINPIN     140\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE']\n",
      "Labels:  tensor([0, 3, 3,  ..., 4, 0, 3])\n",
      "Total count:  2478\n",
      "\n",
      "Validation Dataset (should match):\n",
      "QUEFAG    1116\n",
      "PINNIG     581\n",
      "QUEILE     364\n",
      "PINSYL     277\n",
      "PINPIN     140\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE']\n",
      "Labels:  tensor([0, 3, 3,  ..., 4, 0, 3])\n",
      "Total count:  2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for specie in list(set(trees_data.species) - set(config.species)):\n",
    "    print(\"Removing: {}\".format(specie))\n",
    "    trees_data.remove_species(specie)\n",
    "    val_data.remove_species(specie)\n",
    "\n",
    "print('Train Dataset:')\n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))\n",
    "print()\n",
    "\n",
    "print('Validation Dataset (should match):')\n",
    "print(val_data.counts)\n",
    "print('Species: ', val_data.species)\n",
    "print('Labels: ', val_data.labels)\n",
    "print('Total count: ', len(val_data))\n",
    "print()\n",
    "\n",
    "assert len(val_data) == len(trees_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b1f4b",
   "metadata": {},
   "source": [
    "#### Train-validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7df7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(trees_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(config.validation_split * dataset_size))\n",
    "\n",
    "if config.shuffle_dataset :\n",
    "    \n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2342e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "val_data.set_params(transforms=['none']) #Turn off transforms for the validation dataset - DON'T GIVE IT AN EMPTY LIST\n",
    "validation_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcb1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to print no. instances of each class\n",
    "if None:\n",
    "    for x in train_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()\n",
    "\n",
    "    for x in validation_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42aa06",
   "metadata": {},
   "source": [
    "### Define model, loss fn, optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e976fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(config.species) == set(trees_data.species)\n",
    "\n",
    "if config.model==\"SimpleView\":\n",
    "    model = SimpleView(\n",
    "        num_views=config.num_views,\n",
    "        num_classes=len(config.species)\n",
    "    )\n",
    "\n",
    "model = model.to(device=device)\n",
    "\n",
    "if config.loss_fn==\"cross-entropy\":\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "if config.loss_fn==\"smooth-loss\":\n",
    "    loss_fn = utils.smooth_loss\n",
    "\n",
    "if type(config.learning_rate) == list:\n",
    "    lr = config.learning_rate[0]\n",
    "    step_size = config.learning_rate[1]\n",
    "    gamma = config.learning_rate[2]\n",
    "else:\n",
    "    lr = config.learning_rate\n",
    "    \n",
    "if config.optimizer==\"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=config.momentum)\n",
    "elif config.optimizer==\"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "if type(config.learning_rate) == list:\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc399a1",
   "metadata": {},
   "source": [
    "### Train & Test Loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f4413fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/Work/MRes-Project/laser-trees/utils/utils.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_world_tilde=torch.cat((torch.tensor(cloud), torch.ones(cloud.shape[0],1)), 1).transpose(0,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 3.429\n",
      "[1,    10] loss: 3.067\n",
      "[1,    15] loss: 2.931\n",
      "Got 76 / 495 with accuracy 15.35\n",
      "[2,     5] loss: 2.863\n",
      "[2,    10] loss: 2.677\n",
      "[2,    15] loss: 2.646\n",
      "Got 124 / 495 with accuracy 25.05\n",
      "[3,     5] loss: 2.471\n",
      "[3,    10] loss: 2.380\n",
      "[3,    15] loss: 2.552\n",
      "Got 155 / 495 with accuracy 31.31\n",
      "[4,     5] loss: 2.315\n",
      "[4,    10] loss: 2.387\n",
      "[4,    15] loss: 2.203\n",
      "Got 163 / 495 with accuracy 32.93\n",
      "[5,     5] loss: 2.354\n",
      "[5,    10] loss: 2.274\n",
      "[5,    15] loss: 2.256\n",
      "Got 161 / 495 with accuracy 32.53\n",
      "[6,     5] loss: 2.205\n",
      "[6,    10] loss: 2.356\n",
      "[6,    15] loss: 2.103\n",
      "Got 341 / 495 with accuracy 68.89\n",
      "[7,     5] loss: 2.132\n",
      "[7,    10] loss: 2.149\n",
      "[7,    15] loss: 2.226\n",
      "Got 279 / 495 with accuracy 56.36\n",
      "[8,     5] loss: 2.069\n",
      "[8,    10] loss: 2.161\n",
      "[8,    15] loss: 2.217\n",
      "Got 327 / 495 with accuracy 66.06\n",
      "[9,     5] loss: 2.215\n",
      "[9,    10] loss: 2.221\n",
      "[9,    15] loss: 1.999\n",
      "Got 329 / 495 with accuracy 66.46\n",
      "[10,     5] loss: 2.155\n",
      "[10,    10] loss: 2.042\n",
      "[10,    15] loss: 2.010\n",
      "Got 249 / 495 with accuracy 50.30\n",
      "[11,     5] loss: 2.097\n",
      "[11,    10] loss: 2.047\n",
      "[11,    15] loss: 2.160\n",
      "Got 229 / 495 with accuracy 46.26\n",
      "[12,     5] loss: 2.159\n",
      "[12,    10] loss: 2.048\n",
      "[12,    15] loss: 1.946\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[13,     5] loss: 1.942\n",
      "[13,    10] loss: 2.023\n",
      "[13,    15] loss: 2.031\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[14,     5] loss: 1.928\n",
      "[14,    10] loss: 2.052\n",
      "[14,    15] loss: 2.100\n",
      "Got 322 / 495 with accuracy 65.05\n",
      "[15,     5] loss: 1.884\n",
      "[15,    10] loss: 1.985\n",
      "[15,    15] loss: 2.004\n",
      "Got 328 / 495 with accuracy 66.26\n",
      "[16,     5] loss: 1.835\n",
      "[16,    10] loss: 2.033\n",
      "[16,    15] loss: 2.003\n",
      "Got 269 / 495 with accuracy 54.34\n",
      "[17,     5] loss: 1.897\n",
      "[17,    10] loss: 2.025\n",
      "[17,    15] loss: 1.870\n",
      "Got 225 / 495 with accuracy 45.45\n",
      "[18,     5] loss: 2.004\n",
      "[18,    10] loss: 1.850\n",
      "[18,    15] loss: 1.977\n",
      "Got 223 / 495 with accuracy 45.05\n",
      "[19,     5] loss: 1.881\n",
      "[19,    10] loss: 2.024\n",
      "[19,    15] loss: 1.943\n",
      "Got 326 / 495 with accuracy 65.86\n",
      "[20,     5] loss: 1.974\n",
      "[20,    10] loss: 1.941\n",
      "[20,    15] loss: 1.737\n",
      "Got 345 / 495 with accuracy 69.70\n",
      "[21,     5] loss: 1.733\n",
      "[21,    10] loss: 1.940\n",
      "[21,    15] loss: 1.824\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[22,     5] loss: 1.895\n",
      "[22,    10] loss: 1.927\n",
      "[22,    15] loss: 1.710\n",
      "Got 283 / 495 with accuracy 57.17\n",
      "[23,     5] loss: 1.881\n",
      "[23,    10] loss: 1.914\n",
      "[23,    15] loss: 1.724\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[24,     5] loss: 1.839\n",
      "[24,    10] loss: 1.865\n",
      "[24,    15] loss: 1.808\n",
      "Got 132 / 495 with accuracy 26.67\n",
      "[25,     5] loss: 1.826\n",
      "[25,    10] loss: 1.822\n",
      "[25,    15] loss: 1.741\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[26,     5] loss: 1.807\n",
      "[26,    10] loss: 1.881\n",
      "[26,    15] loss: 1.798\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[27,     5] loss: 1.737\n",
      "[27,    10] loss: 1.708\n",
      "[27,    15] loss: 1.887\n",
      "Got 300 / 495 with accuracy 60.61\n",
      "[28,     5] loss: 1.741\n",
      "[28,    10] loss: 1.692\n",
      "[28,    15] loss: 1.883\n",
      "Got 285 / 495 with accuracy 57.58\n",
      "[29,     5] loss: 1.896\n",
      "[29,    10] loss: 1.608\n",
      "[29,    15] loss: 1.836\n",
      "Got 323 / 495 with accuracy 65.25\n",
      "[30,     5] loss: 1.790\n",
      "[30,    10] loss: 1.691\n",
      "[30,    15] loss: 1.705\n",
      "Got 358 / 495 with accuracy 72.32\n",
      "[31,     5] loss: 1.635\n",
      "[31,    10] loss: 1.766\n",
      "[31,    15] loss: 1.673\n",
      "Got 214 / 495 with accuracy 43.23\n",
      "[32,     5] loss: 1.765\n",
      "[32,    10] loss: 1.602\n",
      "[32,    15] loss: 1.686\n",
      "Got 292 / 495 with accuracy 58.99\n",
      "[33,     5] loss: 1.739\n",
      "[33,    10] loss: 1.584\n",
      "[33,    15] loss: 1.696\n",
      "Got 193 / 495 with accuracy 38.99\n",
      "[34,     5] loss: 1.648\n",
      "[34,    10] loss: 1.648\n",
      "[34,    15] loss: 1.793\n",
      "Got 356 / 495 with accuracy 71.92\n",
      "[35,     5] loss: 1.700\n",
      "[35,    10] loss: 1.705\n",
      "[35,    15] loss: 1.634\n",
      "Got 179 / 495 with accuracy 36.16\n",
      "[36,     5] loss: 1.662\n",
      "[36,    10] loss: 1.741\n",
      "[36,    15] loss: 1.595\n",
      "Got 322 / 495 with accuracy 65.05\n",
      "[37,     5] loss: 1.564\n",
      "[37,    10] loss: 1.588\n",
      "[37,    15] loss: 1.809\n",
      "Got 322 / 495 with accuracy 65.05\n",
      "[38,     5] loss: 1.610\n",
      "[38,    10] loss: 1.687\n",
      "[38,    15] loss: 1.507\n",
      "Got 372 / 495 with accuracy 75.15\n",
      "[39,     5] loss: 1.545\n",
      "[39,    10] loss: 1.637\n",
      "[39,    15] loss: 1.607\n",
      "Got 336 / 495 with accuracy 67.88\n",
      "[40,     5] loss: 1.645\n",
      "[40,    10] loss: 1.541\n",
      "[40,    15] loss: 1.612\n",
      "Got 306 / 495 with accuracy 61.82\n",
      "[41,     5] loss: 1.505\n",
      "[41,    10] loss: 1.516\n",
      "[41,    15] loss: 1.654\n",
      "Got 358 / 495 with accuracy 72.32\n",
      "[42,     5] loss: 1.620\n",
      "[42,    10] loss: 1.536\n",
      "[42,    15] loss: 1.500\n",
      "Got 353 / 495 with accuracy 71.31\n",
      "[43,     5] loss: 1.634\n",
      "[43,    10] loss: 1.629\n",
      "[43,    15] loss: 1.618\n",
      "Got 270 / 495 with accuracy 54.55\n",
      "[44,     5] loss: 1.598\n",
      "[44,    10] loss: 1.513\n",
      "[44,    15] loss: 1.595\n",
      "Got 210 / 495 with accuracy 42.42\n",
      "[45,     5] loss: 1.517\n",
      "[45,    10] loss: 1.493\n",
      "[45,    15] loss: 1.615\n",
      "Got 302 / 495 with accuracy 61.01\n",
      "[46,     5] loss: 1.420\n",
      "[46,    10] loss: 1.571\n",
      "[46,    15] loss: 1.508\n",
      "Got 358 / 495 with accuracy 72.32\n",
      "[47,     5] loss: 1.408\n",
      "[47,    10] loss: 1.546\n",
      "[47,    15] loss: 1.318\n",
      "Got 328 / 495 with accuracy 66.26\n",
      "[48,     5] loss: 1.616\n",
      "[48,    10] loss: 1.602\n",
      "[48,    15] loss: 1.419\n",
      "Got 326 / 495 with accuracy 65.86\n",
      "[49,     5] loss: 1.500\n",
      "[49,    10] loss: 1.605\n",
      "[49,    15] loss: 1.469\n",
      "Got 237 / 495 with accuracy 47.88\n",
      "[50,     5] loss: 1.506\n",
      "[50,    10] loss: 1.413\n",
      "[50,    15] loss: 1.436\n",
      "Got 328 / 495 with accuracy 66.26\n",
      "[51,     5] loss: 1.415\n",
      "[51,    10] loss: 1.486\n",
      "[51,    15] loss: 1.491\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[52,     5] loss: 1.481\n",
      "[52,    10] loss: 1.519\n",
      "[52,    15] loss: 1.356\n",
      "Got 314 / 495 with accuracy 63.43\n",
      "[53,     5] loss: 1.558\n",
      "[53,    10] loss: 1.426\n",
      "[53,    15] loss: 1.265\n",
      "Got 365 / 495 with accuracy 73.74\n",
      "[54,     5] loss: 1.360\n",
      "[54,    10] loss: 1.470\n",
      "[54,    15] loss: 1.457\n",
      "Got 284 / 495 with accuracy 57.37\n",
      "[55,     5] loss: 1.293\n",
      "[55,    10] loss: 1.371\n",
      "[55,    15] loss: 1.503\n",
      "Got 320 / 495 with accuracy 64.65\n",
      "[56,     5] loss: 1.343\n",
      "[56,    10] loss: 1.381\n",
      "[56,    15] loss: 1.558\n",
      "Got 258 / 495 with accuracy 52.12\n",
      "[57,     5] loss: 1.366\n",
      "[57,    10] loss: 1.264\n",
      "[57,    15] loss: 1.431\n",
      "Got 356 / 495 with accuracy 71.92\n",
      "[58,     5] loss: 1.270\n",
      "[58,    10] loss: 1.344\n",
      "[58,    15] loss: 1.343\n",
      "Got 346 / 495 with accuracy 69.90\n",
      "[59,     5] loss: 1.406\n",
      "[59,    10] loss: 1.400\n",
      "[59,    15] loss: 1.337\n",
      "Got 297 / 495 with accuracy 60.00\n",
      "[60,     5] loss: 1.467\n",
      "[60,    10] loss: 1.325\n",
      "[60,    15] loss: 1.372\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[61,     5] loss: 1.350\n",
      "[61,    10] loss: 1.455\n",
      "[61,    15] loss: 1.367\n",
      "Got 354 / 495 with accuracy 71.52\n",
      "[62,     5] loss: 1.208\n",
      "[62,    10] loss: 1.301\n",
      "[62,    15] loss: 1.429\n",
      "Got 302 / 495 with accuracy 61.01\n",
      "[63,     5] loss: 1.210\n",
      "[63,    10] loss: 1.439\n",
      "[63,    15] loss: 1.242\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[64,     5] loss: 1.418\n",
      "[64,    10] loss: 1.288\n",
      "[64,    15] loss: 1.313\n",
      "Got 372 / 495 with accuracy 75.15\n",
      "[65,     5] loss: 1.330\n",
      "[65,    10] loss: 1.294\n",
      "[65,    15] loss: 1.372\n",
      "Got 314 / 495 with accuracy 63.43\n",
      "[66,     5] loss: 1.297\n",
      "[66,    10] loss: 1.313\n",
      "[66,    15] loss: 1.325\n",
      "Got 333 / 495 with accuracy 67.27\n",
      "[67,     5] loss: 1.336\n",
      "[67,    10] loss: 1.251\n",
      "[67,    15] loss: 1.404\n",
      "Got 360 / 495 with accuracy 72.73\n",
      "[68,     5] loss: 1.355\n",
      "[68,    10] loss: 1.258\n",
      "[68,    15] loss: 1.245\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[69,     5] loss: 1.240\n",
      "[69,    10] loss: 1.124\n",
      "[69,    15] loss: 1.297\n",
      "Got 360 / 495 with accuracy 72.73\n",
      "[70,     5] loss: 1.209\n",
      "[70,    10] loss: 1.200\n",
      "[70,    15] loss: 1.314\n",
      "Got 338 / 495 with accuracy 68.28\n",
      "[71,     5] loss: 1.172\n",
      "[71,    10] loss: 1.272\n",
      "[71,    15] loss: 1.389\n",
      "Got 284 / 495 with accuracy 57.37\n",
      "[72,     5] loss: 1.310\n",
      "[72,    10] loss: 1.294\n",
      "[72,    15] loss: 1.180\n",
      "Got 225 / 495 with accuracy 45.45\n",
      "[73,     5] loss: 1.231\n",
      "[73,    10] loss: 1.245\n",
      "[73,    15] loss: 1.160\n",
      "Got 377 / 495 with accuracy 76.16\n",
      "[74,     5] loss: 1.190\n",
      "[74,    10] loss: 1.359\n",
      "[74,    15] loss: 1.179\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[75,     5] loss: 1.292\n",
      "[75,    10] loss: 1.230\n",
      "[75,    15] loss: 1.235\n",
      "Got 294 / 495 with accuracy 59.39\n",
      "[76,     5] loss: 1.144\n",
      "[76,    10] loss: 1.181\n",
      "[76,    15] loss: 1.314\n",
      "Got 385 / 495 with accuracy 77.78\n",
      "[77,     5] loss: 1.187\n",
      "[77,    10] loss: 1.209\n",
      "[77,    15] loss: 1.130\n",
      "Got 356 / 495 with accuracy 71.92\n",
      "[78,     5] loss: 1.230\n",
      "[78,    10] loss: 1.274\n",
      "[78,    15] loss: 1.376\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[79,     5] loss: 1.180\n",
      "[79,    10] loss: 1.255\n",
      "[79,    15] loss: 1.208\n",
      "Got 371 / 495 with accuracy 74.95\n",
      "[80,     5] loss: 1.139\n",
      "[80,    10] loss: 1.151\n",
      "[80,    15] loss: 1.199\n",
      "Got 355 / 495 with accuracy 71.72\n",
      "[81,     5] loss: 1.217\n",
      "[81,    10] loss: 1.074\n",
      "[81,    15] loss: 1.271\n",
      "Got 379 / 495 with accuracy 76.57\n",
      "[82,     5] loss: 1.076\n",
      "[82,    10] loss: 1.132\n",
      "[82,    15] loss: 1.220\n",
      "Got 364 / 495 with accuracy 73.54\n",
      "[83,     5] loss: 1.081\n",
      "[83,    10] loss: 1.302\n",
      "[83,    15] loss: 1.124\n",
      "Got 341 / 495 with accuracy 68.89\n",
      "[84,     5] loss: 1.188\n",
      "[84,    10] loss: 1.155\n",
      "[84,    15] loss: 1.094\n",
      "Got 360 / 495 with accuracy 72.73\n",
      "[85,     5] loss: 1.130\n",
      "[85,    10] loss: 1.107\n",
      "[85,    15] loss: 1.164\n",
      "Got 303 / 495 with accuracy 61.21\n",
      "[86,     5] loss: 1.381\n",
      "[86,    10] loss: 1.122\n",
      "[86,    15] loss: 0.980\n",
      "Got 300 / 495 with accuracy 60.61\n",
      "[87,     5] loss: 1.164\n",
      "[87,    10] loss: 1.204\n",
      "[87,    15] loss: 1.193\n",
      "Got 328 / 495 with accuracy 66.26\n",
      "[88,     5] loss: 1.253\n",
      "[88,    10] loss: 1.044\n",
      "[88,    15] loss: 1.090\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[89,     5] loss: 1.019\n",
      "[89,    10] loss: 1.182\n",
      "[89,    15] loss: 1.117\n",
      "Got 265 / 495 with accuracy 53.54\n",
      "[90,     5] loss: 1.098\n",
      "[90,    10] loss: 1.140\n",
      "[90,    15] loss: 1.104\n",
      "Got 211 / 495 with accuracy 42.63\n",
      "[91,     5] loss: 1.126\n",
      "[91,    10] loss: 0.978\n",
      "[91,    15] loss: 1.150\n",
      "Got 354 / 495 with accuracy 71.52\n",
      "[92,     5] loss: 1.043\n",
      "[92,    10] loss: 1.065\n",
      "[92,    15] loss: 1.175\n",
      "Got 292 / 495 with accuracy 58.99\n",
      "[93,     5] loss: 1.108\n",
      "[93,    10] loss: 1.080\n",
      "[93,    15] loss: 1.040\n",
      "Got 262 / 495 with accuracy 52.93\n",
      "[94,     5] loss: 0.973\n",
      "[94,    10] loss: 1.223\n",
      "[94,    15] loss: 1.152\n",
      "Got 306 / 495 with accuracy 61.82\n",
      "[95,     5] loss: 1.135\n",
      "[95,    10] loss: 1.105\n",
      "[95,    15] loss: 1.007\n",
      "Got 348 / 495 with accuracy 70.30\n",
      "[96,     5] loss: 1.065\n",
      "[96,    10] loss: 1.086\n",
      "[96,    15] loss: 1.159\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[97,     5] loss: 0.906\n",
      "[97,    10] loss: 1.206\n",
      "[97,    15] loss: 1.010\n",
      "Got 217 / 495 with accuracy 43.84\n",
      "[98,     5] loss: 0.962\n",
      "[98,    10] loss: 0.973\n",
      "[98,    15] loss: 1.041\n",
      "Got 356 / 495 with accuracy 71.92\n",
      "[99,     5] loss: 0.972\n",
      "[99,    10] loss: 1.046\n",
      "[99,    15] loss: 1.150\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[100,     5] loss: 0.985\n",
      "[100,    10] loss: 1.066\n",
      "[100,    15] loss: 1.066\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[101,     5] loss: 1.033\n",
      "[101,    10] loss: 0.965\n",
      "[101,    15] loss: 0.897\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[102,     5] loss: 0.926\n",
      "[102,    10] loss: 1.049\n",
      "[102,    15] loss: 0.914\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[103,     5] loss: 0.911\n",
      "[103,    10] loss: 0.879\n",
      "[103,    15] loss: 0.939\n",
      "Got 393 / 495 with accuracy 79.39\n",
      "[104,     5] loss: 0.968\n",
      "[104,    10] loss: 0.957\n",
      "[104,    15] loss: 0.829\n",
      "Got 388 / 495 with accuracy 78.38\n",
      "[105,     5] loss: 0.875\n",
      "[105,    10] loss: 1.027\n",
      "[105,    15] loss: 0.877\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[106,     5] loss: 0.762\n",
      "[106,    10] loss: 0.998\n",
      "[106,    15] loss: 0.997\n",
      "Got 353 / 495 with accuracy 71.31\n",
      "[107,     5] loss: 0.872\n",
      "[107,    10] loss: 0.964\n",
      "[107,    15] loss: 1.024\n",
      "Got 410 / 495 with accuracy 82.83\n",
      "[108,     5] loss: 0.874\n",
      "[108,    10] loss: 0.818\n",
      "[108,    15] loss: 1.011\n",
      "Got 388 / 495 with accuracy 78.38\n",
      "[109,     5] loss: 0.887\n",
      "[109,    10] loss: 0.983\n",
      "[109,    15] loss: 0.877\n",
      "Got 389 / 495 with accuracy 78.59\n",
      "[110,     5] loss: 0.903\n",
      "[110,    10] loss: 0.953\n",
      "[110,    15] loss: 0.951\n",
      "Got 364 / 495 with accuracy 73.54\n",
      "[111,     5] loss: 0.883\n",
      "[111,    10] loss: 0.987\n",
      "[111,    15] loss: 0.915\n",
      "Got 354 / 495 with accuracy 71.52\n",
      "[112,     5] loss: 0.931\n",
      "[112,    10] loss: 0.869\n",
      "[112,    15] loss: 0.819\n",
      "Got 364 / 495 with accuracy 73.54\n",
      "[113,     5] loss: 0.912\n",
      "[113,    10] loss: 0.871\n",
      "[113,    15] loss: 0.887\n",
      "Got 348 / 495 with accuracy 70.30\n",
      "[114,     5] loss: 0.829\n",
      "[114,    10] loss: 0.979\n",
      "[114,    15] loss: 0.877\n",
      "Got 349 / 495 with accuracy 70.51\n",
      "[115,     5] loss: 0.906\n",
      "[115,    10] loss: 0.807\n",
      "[115,    15] loss: 0.914\n",
      "Got 406 / 495 with accuracy 82.02\n",
      "[116,     5] loss: 0.918\n",
      "[116,    10] loss: 0.839\n",
      "[116,    15] loss: 0.837\n",
      "Got 388 / 495 with accuracy 78.38\n",
      "[117,     5] loss: 0.740\n",
      "[117,    10] loss: 0.907\n",
      "[117,    15] loss: 0.888\n",
      "Got 375 / 495 with accuracy 75.76\n",
      "[118,     5] loss: 0.861\n",
      "[118,    10] loss: 0.847\n",
      "[118,    15] loss: 0.751\n",
      "Got 363 / 495 with accuracy 73.33\n",
      "[119,     5] loss: 0.787\n",
      "[119,    10] loss: 0.745\n",
      "[119,    15] loss: 0.970\n",
      "Got 361 / 495 with accuracy 72.93\n",
      "[120,     5] loss: 0.860\n",
      "[120,    10] loss: 0.863\n",
      "[120,    15] loss: 0.944\n",
      "Got 355 / 495 with accuracy 71.72\n",
      "[121,     5] loss: 0.753\n",
      "[121,    10] loss: 0.873\n",
      "[121,    15] loss: 0.918\n",
      "Got 380 / 495 with accuracy 76.77\n",
      "[122,     5] loss: 0.826\n",
      "[122,    10] loss: 0.914\n",
      "[122,    15] loss: 0.824\n",
      "Got 362 / 495 with accuracy 73.13\n",
      "[123,     5] loss: 0.794\n",
      "[123,    10] loss: 0.792\n",
      "[123,    15] loss: 0.897\n",
      "Got 295 / 495 with accuracy 59.60\n",
      "[124,     5] loss: 0.830\n",
      "[124,    10] loss: 0.784\n",
      "[124,    15] loss: 0.931\n",
      "Got 413 / 495 with accuracy 83.43\n",
      "[125,     5] loss: 0.829\n",
      "[125,    10] loss: 0.857\n",
      "[125,    15] loss: 0.738\n",
      "Got 387 / 495 with accuracy 78.18\n",
      "[126,     5] loss: 0.813\n",
      "[126,    10] loss: 0.802\n",
      "[126,    15] loss: 0.897\n",
      "Got 385 / 495 with accuracy 77.78\n",
      "[127,     5] loss: 0.827\n",
      "[127,    10] loss: 0.849\n",
      "[127,    15] loss: 0.915\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[128,     5] loss: 0.790\n",
      "[128,    10] loss: 0.752\n",
      "[128,    15] loss: 0.881\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[129,     5] loss: 0.817\n",
      "[129,    10] loss: 0.892\n",
      "[129,    15] loss: 0.757\n",
      "Got 356 / 495 with accuracy 71.92\n",
      "[130,     5] loss: 0.839\n",
      "[130,    10] loss: 0.807\n",
      "[130,    15] loss: 0.824\n",
      "Got 337 / 495 with accuracy 68.08\n",
      "[131,     5] loss: 0.788\n",
      "[131,    10] loss: 0.725\n",
      "[131,    15] loss: 0.900\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[132,     5] loss: 0.854\n",
      "[132,    10] loss: 0.894\n",
      "[132,    15] loss: 0.787\n",
      "Got 357 / 495 with accuracy 72.12\n",
      "[133,     5] loss: 0.785\n",
      "[133,    10] loss: 0.780\n",
      "[133,    15] loss: 0.822\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[134,     5] loss: 0.821\n",
      "[134,    10] loss: 0.854\n",
      "[134,    15] loss: 0.747\n",
      "Got 385 / 495 with accuracy 77.78\n",
      "[135,     5] loss: 0.693\n",
      "[135,    10] loss: 0.742\n",
      "[135,    15] loss: 0.876\n",
      "Got 387 / 495 with accuracy 78.18\n",
      "[136,     5] loss: 0.792\n",
      "[136,    10] loss: 0.813\n",
      "[136,    15] loss: 0.776\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[137,     5] loss: 0.680\n",
      "[137,    10] loss: 0.830\n",
      "[137,    15] loss: 0.711\n",
      "Got 393 / 495 with accuracy 79.39\n",
      "[138,     5] loss: 0.802\n",
      "[138,    10] loss: 0.769\n",
      "[138,    15] loss: 0.741\n",
      "Got 378 / 495 with accuracy 76.36\n",
      "[139,     5] loss: 0.841\n",
      "[139,    10] loss: 0.732\n",
      "[139,    15] loss: 0.714\n",
      "Got 397 / 495 with accuracy 80.20\n",
      "[140,     5] loss: 0.667\n",
      "[140,    10] loss: 0.766\n",
      "[140,    15] loss: 0.744\n",
      "Got 365 / 495 with accuracy 73.74\n",
      "[141,     5] loss: 0.667\n",
      "[141,    10] loss: 0.705\n",
      "[141,    15] loss: 0.990\n",
      "Got 389 / 495 with accuracy 78.59\n",
      "[142,     5] loss: 0.800\n",
      "[142,    10] loss: 0.643\n",
      "[142,    15] loss: 0.750\n",
      "Got 377 / 495 with accuracy 76.16\n",
      "[143,     5] loss: 0.781\n",
      "[143,    10] loss: 0.735\n",
      "[143,    15] loss: 0.761\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[144,     5] loss: 0.771\n",
      "[144,    10] loss: 0.808\n",
      "[144,    15] loss: 0.775\n",
      "Got 351 / 495 with accuracy 70.91\n",
      "[145,     5] loss: 0.617\n",
      "[145,    10] loss: 0.770\n",
      "[145,    15] loss: 0.725\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[146,     5] loss: 0.830\n",
      "[146,    10] loss: 0.664\n",
      "[146,    15] loss: 0.905\n",
      "Got 367 / 495 with accuracy 74.14\n",
      "[147,     5] loss: 0.721\n",
      "[147,    10] loss: 0.831\n",
      "[147,    15] loss: 0.706\n",
      "Got 346 / 495 with accuracy 69.90\n",
      "[148,     5] loss: 0.657\n",
      "[148,    10] loss: 0.846\n",
      "[148,    15] loss: 0.664\n",
      "Got 376 / 495 with accuracy 75.96\n",
      "[149,     5] loss: 0.696\n",
      "[149,    10] loss: 0.638\n",
      "[149,    15] loss: 0.702\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[150,     5] loss: 0.777\n",
      "[150,    10] loss: 0.726\n",
      "[150,    15] loss: 0.730\n",
      "Got 338 / 495 with accuracy 68.28\n",
      "[151,     5] loss: 0.734\n",
      "[151,    10] loss: 0.664\n",
      "[151,    15] loss: 0.717\n",
      "Got 367 / 495 with accuracy 74.14\n",
      "[152,     5] loss: 0.619\n",
      "[152,    10] loss: 0.712\n",
      "[152,    15] loss: 0.810\n",
      "Got 354 / 495 with accuracy 71.52\n",
      "[153,     5] loss: 0.715\n",
      "[153,    10] loss: 0.724\n",
      "[153,    15] loss: 0.846\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[154,     5] loss: 0.738\n",
      "[154,    10] loss: 0.683\n",
      "[154,    15] loss: 0.771\n",
      "Got 370 / 495 with accuracy 74.75\n",
      "[155,     5] loss: 0.675\n",
      "[155,    10] loss: 0.763\n",
      "[155,    15] loss: 0.790\n",
      "Got 355 / 495 with accuracy 71.72\n",
      "[156,     5] loss: 0.753\n",
      "[156,    10] loss: 0.675\n",
      "[156,    15] loss: 0.729\n",
      "Got 371 / 495 with accuracy 74.95\n",
      "[157,     5] loss: 0.669\n",
      "[157,    10] loss: 0.722\n",
      "[157,    15] loss: 0.794\n",
      "Got 368 / 495 with accuracy 74.34\n",
      "[158,     5] loss: 0.639\n",
      "[158,    10] loss: 0.823\n",
      "[158,    15] loss: 0.760\n",
      "Got 337 / 495 with accuracy 68.08\n",
      "[159,     5] loss: 0.733\n",
      "[159,    10] loss: 0.735\n",
      "[159,    15] loss: 0.652\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[160,     5] loss: 0.650\n",
      "[160,    10] loss: 0.756\n",
      "[160,    15] loss: 0.682\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[161,     5] loss: 0.631\n",
      "[161,    10] loss: 0.667\n",
      "[161,    15] loss: 0.691\n",
      "Got 405 / 495 with accuracy 81.82\n",
      "[162,     5] loss: 0.567\n",
      "[162,    10] loss: 0.673\n",
      "[162,    15] loss: 0.657\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[163,     5] loss: 0.695\n",
      "[163,    10] loss: 0.700\n",
      "[163,    15] loss: 0.681\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[164,     5] loss: 0.718\n",
      "[164,    10] loss: 0.720\n",
      "[164,    15] loss: 0.654\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[165,     5] loss: 0.651\n",
      "[165,    10] loss: 0.824\n",
      "[165,    15] loss: 0.712\n",
      "Got 347 / 495 with accuracy 70.10\n",
      "[166,     5] loss: 0.611\n",
      "[166,    10] loss: 0.806\n",
      "[166,    15] loss: 0.687\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[167,     5] loss: 0.604\n",
      "[167,    10] loss: 0.727\n",
      "[167,    15] loss: 0.732\n",
      "Got 266 / 495 with accuracy 53.74\n",
      "[168,     5] loss: 0.784\n",
      "[168,    10] loss: 0.599\n",
      "[168,    15] loss: 0.751\n",
      "Got 393 / 495 with accuracy 79.39\n",
      "[169,     5] loss: 0.760\n",
      "[169,    10] loss: 0.781\n",
      "[169,    15] loss: 0.690\n",
      "Got 386 / 495 with accuracy 77.98\n",
      "[170,     5] loss: 0.629\n",
      "[170,    10] loss: 0.738\n",
      "[170,    15] loss: 0.693\n",
      "Got 373 / 495 with accuracy 75.35\n",
      "[171,     5] loss: 0.675\n",
      "[171,    10] loss: 0.657\n",
      "[171,    15] loss: 0.655\n",
      "Got 335 / 495 with accuracy 67.68\n",
      "[172,     5] loss: 0.647\n",
      "[172,    10] loss: 0.691\n",
      "[172,    15] loss: 0.663\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[173,     5] loss: 0.634\n",
      "[173,    10] loss: 0.704\n",
      "[173,    15] loss: 0.588\n",
      "Got 362 / 495 with accuracy 73.13\n",
      "[174,     5] loss: 0.663\n",
      "[174,    10] loss: 0.726\n",
      "[174,    15] loss: 0.596\n",
      "Got 367 / 495 with accuracy 74.14\n",
      "[175,     5] loss: 0.636\n",
      "[175,    10] loss: 0.632\n",
      "[175,    15] loss: 0.705\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[176,     5] loss: 0.669\n",
      "[176,    10] loss: 0.616\n",
      "[176,    15] loss: 0.651\n",
      "Got 388 / 495 with accuracy 78.38\n",
      "[177,     5] loss: 0.516\n",
      "[177,    10] loss: 0.606\n",
      "[177,    15] loss: 0.637\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[178,     5] loss: 0.689\n",
      "[178,    10] loss: 0.628\n",
      "[178,    15] loss: 0.703\n",
      "Got 368 / 495 with accuracy 74.34\n",
      "[179,     5] loss: 0.603\n",
      "[179,    10] loss: 0.614\n",
      "[179,    15] loss: 0.785\n",
      "Got 392 / 495 with accuracy 79.19\n",
      "[180,     5] loss: 0.552\n",
      "[180,    10] loss: 0.683\n",
      "[180,    15] loss: 0.598\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[181,     5] loss: 0.665\n",
      "[181,    10] loss: 0.662\n",
      "[181,    15] loss: 0.673\n",
      "Got 380 / 495 with accuracy 76.77\n",
      "[182,     5] loss: 0.555\n",
      "[182,    10] loss: 0.653\n",
      "[182,    15] loss: 0.676\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[183,     5] loss: 0.502\n",
      "[183,    10] loss: 0.649\n",
      "[183,    15] loss: 0.568\n",
      "Got 385 / 495 with accuracy 77.78\n",
      "[184,     5] loss: 0.645\n",
      "[184,    10] loss: 0.626\n",
      "[184,    15] loss: 0.617\n",
      "Got 369 / 495 with accuracy 74.55\n",
      "[185,     5] loss: 0.625\n",
      "[185,    10] loss: 0.574\n",
      "[185,    15] loss: 0.676\n",
      "Got 392 / 495 with accuracy 79.19\n",
      "[186,     5] loss: 0.645\n",
      "[186,    10] loss: 0.670\n",
      "[186,    15] loss: 0.546\n",
      "Got 333 / 495 with accuracy 67.27\n",
      "[187,     5] loss: 0.630\n",
      "[187,    10] loss: 0.654\n",
      "[187,    15] loss: 0.643\n",
      "Got 382 / 495 with accuracy 77.17\n",
      "[188,     5] loss: 0.666\n",
      "[188,    10] loss: 0.548\n",
      "[188,    15] loss: 0.712\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[189,     5] loss: 0.625\n",
      "[189,    10] loss: 0.589\n",
      "[189,    15] loss: 0.671\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[190,     5] loss: 0.605\n",
      "[190,    10] loss: 0.625\n",
      "[190,    15] loss: 0.594\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[191,     5] loss: 0.490\n",
      "[191,    10] loss: 0.673\n",
      "[191,    15] loss: 0.673\n",
      "Got 316 / 495 with accuracy 63.84\n",
      "[192,     5] loss: 0.648\n",
      "[192,    10] loss: 0.522\n",
      "[192,    15] loss: 0.656\n",
      "Got 389 / 495 with accuracy 78.59\n",
      "[193,     5] loss: 0.477\n",
      "[193,    10] loss: 0.566\n",
      "[193,    15] loss: 0.518\n",
      "Got 324 / 495 with accuracy 65.45\n",
      "[194,     5] loss: 0.609\n",
      "[194,    10] loss: 0.502\n",
      "[194,    15] loss: 0.598\n",
      "Got 381 / 495 with accuracy 76.97\n",
      "[195,     5] loss: 0.534\n",
      "[195,    10] loss: 0.472\n",
      "[195,    15] loss: 0.578\n",
      "Got 397 / 495 with accuracy 80.20\n",
      "[196,     5] loss: 0.622\n",
      "[196,    10] loss: 0.591\n",
      "[196,    15] loss: 0.499\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[197,     5] loss: 0.669\n",
      "[197,    10] loss: 0.597\n",
      "[197,    15] loss: 0.646\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[198,     5] loss: 0.599\n",
      "[198,    10] loss: 0.654\n",
      "[198,    15] loss: 0.581\n",
      "Got 378 / 495 with accuracy 76.36\n",
      "[199,     5] loss: 0.581\n",
      "[199,    10] loss: 0.625\n",
      "[199,    15] loss: 0.534\n",
      "Got 392 / 495 with accuracy 79.19\n",
      "[200,     5] loss: 0.574\n",
      "[200,    10] loss: 0.633\n",
      "[200,    15] loss: 0.524\n",
      "Got 360 / 495 with accuracy 72.73\n",
      "[201,     5] loss: 0.551\n",
      "[201,    10] loss: 0.558\n",
      "[201,    15] loss: 0.505\n",
      "Got 406 / 495 with accuracy 82.02\n",
      "[202,     5] loss: 0.571\n",
      "[202,    10] loss: 0.564\n",
      "[202,    15] loss: 0.505\n",
      "Got 399 / 495 with accuracy 80.61\n",
      "[203,     5] loss: 0.509\n",
      "[203,    10] loss: 0.460\n",
      "[203,    15] loss: 0.519\n",
      "Got 406 / 495 with accuracy 82.02\n",
      "[204,     5] loss: 0.468\n",
      "[204,    10] loss: 0.538\n",
      "[204,    15] loss: 0.539\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[205,     5] loss: 0.501\n",
      "[205,    10] loss: 0.487\n",
      "[205,    15] loss: 0.496\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[206,     5] loss: 0.447\n",
      "[206,    10] loss: 0.489\n",
      "[206,    15] loss: 0.461\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[207,     5] loss: 0.471\n",
      "[207,    10] loss: 0.513\n",
      "[207,    15] loss: 0.480\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[208,     5] loss: 0.513\n",
      "[208,    10] loss: 0.635\n",
      "[208,    15] loss: 0.426\n",
      "Got 387 / 495 with accuracy 78.18\n",
      "[209,     5] loss: 0.466\n",
      "[209,    10] loss: 0.469\n",
      "[209,    15] loss: 0.397\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[210,     5] loss: 0.455\n",
      "[210,    10] loss: 0.441\n",
      "[210,    15] loss: 0.547\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[211,     5] loss: 0.466\n",
      "[211,    10] loss: 0.562\n",
      "[211,    15] loss: 0.508\n",
      "Got 376 / 495 with accuracy 75.96\n",
      "[212,     5] loss: 0.444\n",
      "[212,    10] loss: 0.524\n",
      "[212,    15] loss: 0.487\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[213,     5] loss: 0.415\n",
      "[213,    10] loss: 0.445\n",
      "[213,    15] loss: 0.528\n",
      "Got 405 / 495 with accuracy 81.82\n",
      "[214,     5] loss: 0.459\n",
      "[214,    10] loss: 0.494\n",
      "[214,    15] loss: 0.466\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[215,     5] loss: 0.487\n",
      "[215,    10] loss: 0.508\n",
      "[215,    15] loss: 0.492\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[216,     5] loss: 0.439\n",
      "[216,    10] loss: 0.450\n",
      "[216,    15] loss: 0.435\n",
      "Got 405 / 495 with accuracy 81.82\n",
      "[217,     5] loss: 0.476\n",
      "[217,    10] loss: 0.441\n",
      "[217,    15] loss: 0.454\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[218,     5] loss: 0.419\n",
      "[218,    10] loss: 0.464\n",
      "[218,    15] loss: 0.474\n",
      "Got 408 / 495 with accuracy 82.42\n",
      "[219,     5] loss: 0.472\n",
      "[219,    10] loss: 0.467\n",
      "[219,    15] loss: 0.504\n",
      "Got 399 / 495 with accuracy 80.61\n",
      "[220,     5] loss: 0.432\n",
      "[220,    10] loss: 0.514\n",
      "[220,    15] loss: 0.372\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[221,     5] loss: 0.342\n",
      "[221,    10] loss: 0.473\n",
      "[221,    15] loss: 0.487\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[222,     5] loss: 0.388\n",
      "[222,    10] loss: 0.419\n",
      "[222,    15] loss: 0.440\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[223,     5] loss: 0.427\n",
      "[223,    10] loss: 0.464\n",
      "[223,    15] loss: 0.422\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[224,     5] loss: 0.506\n",
      "[224,    10] loss: 0.457\n",
      "[224,    15] loss: 0.379\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[225,     5] loss: 0.424\n",
      "[225,    10] loss: 0.470\n",
      "[225,    15] loss: 0.455\n",
      "Got 404 / 495 with accuracy 81.62\n",
      "[226,     5] loss: 0.509\n",
      "[226,    10] loss: 0.462\n",
      "[226,    15] loss: 0.523\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[227,     5] loss: 0.418\n",
      "[227,    10] loss: 0.409\n",
      "[227,    15] loss: 0.344\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[228,     5] loss: 0.483\n",
      "[228,    10] loss: 0.401\n",
      "[228,    15] loss: 0.447\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[229,     5] loss: 0.395\n",
      "[229,    10] loss: 0.510\n",
      "[229,    15] loss: 0.515\n",
      "Got 386 / 495 with accuracy 77.98\n",
      "[230,     5] loss: 0.475\n",
      "[230,    10] loss: 0.364\n",
      "[230,    15] loss: 0.406\n",
      "Got 405 / 495 with accuracy 81.82\n",
      "[231,     5] loss: 0.415\n",
      "[231,    10] loss: 0.398\n",
      "[231,    15] loss: 0.452\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[232,     5] loss: 0.334\n",
      "[232,    10] loss: 0.426\n",
      "[232,    15] loss: 0.456\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[233,     5] loss: 0.469\n",
      "[233,    10] loss: 0.437\n",
      "[233,    15] loss: 0.378\n",
      "Got 409 / 495 with accuracy 82.63\n",
      "[234,     5] loss: 0.398\n",
      "[234,    10] loss: 0.453\n",
      "[234,    15] loss: 0.382\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[235,     5] loss: 0.396\n",
      "[235,    10] loss: 0.447\n",
      "[235,    15] loss: 0.412\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[236,     5] loss: 0.438\n",
      "[236,    10] loss: 0.467\n",
      "[236,    15] loss: 0.429\n",
      "Got 369 / 495 with accuracy 74.55\n",
      "[237,     5] loss: 0.412\n",
      "[237,    10] loss: 0.437\n",
      "[237,    15] loss: 0.433\n",
      "Got 370 / 495 with accuracy 74.75\n",
      "[238,     5] loss: 0.364\n",
      "[238,    10] loss: 0.448\n",
      "[238,    15] loss: 0.445\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[239,     5] loss: 0.373\n",
      "[239,    10] loss: 0.407\n",
      "[239,    15] loss: 0.405\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[240,     5] loss: 0.423\n",
      "[240,    10] loss: 0.407\n",
      "[240,    15] loss: 0.419\n",
      "Got 381 / 495 with accuracy 76.97\n",
      "[241,     5] loss: 0.371\n",
      "[241,    10] loss: 0.414\n",
      "[241,    15] loss: 0.485\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[242,     5] loss: 0.378\n",
      "[242,    10] loss: 0.392\n",
      "[242,    15] loss: 0.481\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[243,     5] loss: 0.469\n",
      "[243,    10] loss: 0.448\n",
      "[243,    15] loss: 0.496\n",
      "Got 393 / 495 with accuracy 79.39\n",
      "[244,     5] loss: 0.467\n",
      "[244,    10] loss: 0.503\n",
      "[244,    15] loss: 0.455\n",
      "Got 399 / 495 with accuracy 80.61\n",
      "[245,     5] loss: 0.367\n",
      "[245,    10] loss: 0.386\n",
      "[245,    15] loss: 0.432\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[246,     5] loss: 0.392\n",
      "[246,    10] loss: 0.400\n",
      "[246,    15] loss: 0.416\n",
      "Got 406 / 495 with accuracy 82.02\n",
      "[247,     5] loss: 0.464\n",
      "[247,    10] loss: 0.387\n",
      "[247,    15] loss: 0.366\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[248,     5] loss: 0.470\n",
      "[248,    10] loss: 0.426\n",
      "[248,    15] loss: 0.401\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[249,     5] loss: 0.338\n",
      "[249,    10] loss: 0.441\n",
      "[249,    15] loss: 0.326\n",
      "Got 374 / 495 with accuracy 75.56\n",
      "[250,     5] loss: 0.479\n",
      "[250,    10] loss: 0.419\n",
      "[250,    15] loss: 0.474\n",
      "Got 401 / 495 with accuracy 81.01\n",
      "[251,     5] loss: 0.380\n",
      "[251,    10] loss: 0.426\n",
      "[251,    15] loss: 0.460\n",
      "Got 383 / 495 with accuracy 77.37\n",
      "[252,     5] loss: 0.386\n",
      "[252,    10] loss: 0.448\n",
      "[252,    15] loss: 0.399\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[253,     5] loss: 0.391\n",
      "[253,    10] loss: 0.480\n",
      "[253,    15] loss: 0.401\n",
      "Got 400 / 495 with accuracy 80.81\n",
      "[254,     5] loss: 0.455\n",
      "[254,    10] loss: 0.353\n",
      "[254,    15] loss: 0.451\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[255,     5] loss: 0.454\n",
      "[255,    10] loss: 0.358\n",
      "[255,    15] loss: 0.425\n",
      "Got 380 / 495 with accuracy 76.77\n",
      "[256,     5] loss: 0.357\n",
      "[256,    10] loss: 0.464\n",
      "[256,    15] loss: 0.303\n",
      "Got 386 / 495 with accuracy 77.98\n",
      "[257,     5] loss: 0.393\n",
      "[257,    10] loss: 0.382\n",
      "[257,    15] loss: 0.378\n",
      "Got 420 / 495 with accuracy 84.85\n",
      "[258,     5] loss: 0.349\n",
      "[258,    10] loss: 0.441\n",
      "[258,    15] loss: 0.415\n",
      "Got 375 / 495 with accuracy 75.76\n",
      "[259,     5] loss: 0.310\n",
      "[259,    10] loss: 0.392\n",
      "[259,    15] loss: 0.381\n",
      "Got 399 / 495 with accuracy 80.61\n",
      "[260,     5] loss: 0.379\n",
      "[260,    10] loss: 0.451\n",
      "[260,    15] loss: 0.359\n",
      "Got 399 / 495 with accuracy 80.61\n",
      "[261,     5] loss: 0.347\n",
      "[261,    10] loss: 0.425\n",
      "[261,    15] loss: 0.332\n",
      "Got 376 / 495 with accuracy 75.96\n",
      "[262,     5] loss: 0.406\n",
      "[262,    10] loss: 0.438\n",
      "[262,    15] loss: 0.396\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[263,     5] loss: 0.404\n",
      "[263,    10] loss: 0.434\n",
      "[263,    15] loss: 0.336\n",
      "Got 391 / 495 with accuracy 78.99\n",
      "[264,     5] loss: 0.367\n",
      "[264,    10] loss: 0.347\n",
      "[264,    15] loss: 0.337\n",
      "Got 379 / 495 with accuracy 76.57\n",
      "[265,     5] loss: 0.312\n",
      "[265,    10] loss: 0.453\n",
      "[265,    15] loss: 0.333\n",
      "Got 386 / 495 with accuracy 77.98\n",
      "[266,     5] loss: 0.317\n",
      "[266,    10] loss: 0.368\n",
      "[266,    15] loss: 0.399\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[267,     5] loss: 0.355\n",
      "[267,    10] loss: 0.370\n",
      "[267,    15] loss: 0.375\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[268,     5] loss: 0.376\n",
      "[268,    10] loss: 0.334\n",
      "[268,    15] loss: 0.403\n",
      "Got 370 / 495 with accuracy 74.75\n",
      "[269,     5] loss: 0.393\n",
      "[269,    10] loss: 0.288\n",
      "[269,    15] loss: 0.349\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[270,     5] loss: 0.358\n",
      "[270,    10] loss: 0.317\n",
      "[270,    15] loss: 0.331\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[271,     5] loss: 0.333\n",
      "[271,    10] loss: 0.311\n",
      "[271,    15] loss: 0.305\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[272,     5] loss: 0.348\n",
      "[272,    10] loss: 0.376\n",
      "[272,    15] loss: 0.309\n",
      "Got 401 / 495 with accuracy 81.01\n",
      "[273,     5] loss: 0.325\n",
      "[273,    10] loss: 0.280\n",
      "[273,    15] loss: 0.376\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[274,     5] loss: 0.347\n",
      "[274,    10] loss: 0.388\n",
      "[274,    15] loss: 0.359\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[275,     5] loss: 0.374\n",
      "[275,    10] loss: 0.387\n",
      "[275,    15] loss: 0.368\n",
      "Got 397 / 495 with accuracy 80.20\n",
      "[276,     5] loss: 0.343\n",
      "[276,    10] loss: 0.356\n",
      "[276,    15] loss: 0.403\n",
      "Got 414 / 495 with accuracy 83.64\n",
      "[277,     5] loss: 0.297\n",
      "[277,    10] loss: 0.327\n",
      "[277,    15] loss: 0.388\n",
      "Got 392 / 495 with accuracy 79.19\n",
      "[278,     5] loss: 0.467\n",
      "[278,    10] loss: 0.304\n",
      "[278,    15] loss: 0.352\n",
      "Got 402 / 495 with accuracy 81.21\n",
      "[279,     5] loss: 0.336\n",
      "[279,    10] loss: 0.336\n",
      "[279,    15] loss: 0.341\n",
      "Got 382 / 495 with accuracy 77.17\n",
      "[280,     5] loss: 0.312\n",
      "[280,    10] loss: 0.398\n",
      "[280,    15] loss: 0.320\n",
      "Got 396 / 495 with accuracy 80.00\n",
      "[281,     5] loss: 0.276\n",
      "[281,    10] loss: 0.359\n",
      "[281,    15] loss: 0.380\n",
      "Got 384 / 495 with accuracy 77.58\n",
      "[282,     5] loss: 0.364\n",
      "[282,    10] loss: 0.300\n",
      "[282,    15] loss: 0.455\n",
      "Got 394 / 495 with accuracy 79.60\n",
      "[283,     5] loss: 0.266\n",
      "[283,    10] loss: 0.308\n",
      "[283,    15] loss: 0.407\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[284,     5] loss: 0.349\n",
      "[284,    10] loss: 0.316\n",
      "[284,    15] loss: 0.334\n",
      "Got 409 / 495 with accuracy 82.63\n",
      "[285,     5] loss: 0.308\n",
      "[285,    10] loss: 0.390\n",
      "[285,    15] loss: 0.368\n",
      "Got 404 / 495 with accuracy 81.62\n",
      "[286,     5] loss: 0.309\n",
      "[286,    10] loss: 0.340\n",
      "[286,    15] loss: 0.361\n",
      "Got 389 / 495 with accuracy 78.59\n",
      "[287,     5] loss: 0.410\n",
      "[287,    10] loss: 0.340\n",
      "[287,    15] loss: 0.377\n",
      "Got 412 / 495 with accuracy 83.23\n",
      "[288,     5] loss: 0.354\n",
      "[288,    10] loss: 0.394\n",
      "[288,    15] loss: 0.330\n",
      "Got 397 / 495 with accuracy 80.20\n",
      "[289,     5] loss: 0.329\n",
      "[289,    10] loss: 0.312\n",
      "[289,    15] loss: 0.317\n",
      "Got 382 / 495 with accuracy 77.17\n",
      "[290,     5] loss: 0.367\n",
      "[290,    10] loss: 0.393\n",
      "[290,    15] loss: 0.413\n",
      "Got 387 / 495 with accuracy 78.18\n",
      "[291,     5] loss: 0.306\n",
      "[291,    10] loss: 0.416\n",
      "[291,    15] loss: 0.268\n",
      "Got 401 / 495 with accuracy 81.01\n",
      "[292,     5] loss: 0.360\n",
      "[292,    10] loss: 0.335\n",
      "[292,    15] loss: 0.305\n",
      "Got 398 / 495 with accuracy 80.40\n",
      "[293,     5] loss: 0.367\n",
      "[293,    10] loss: 0.277\n",
      "[293,    15] loss: 0.324\n",
      "Got 390 / 495 with accuracy 78.79\n",
      "[294,     5] loss: 0.364\n",
      "[294,    10] loss: 0.388\n",
      "[294,    15] loss: 0.331\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[295,     5] loss: 0.369\n",
      "[295,    10] loss: 0.386\n",
      "[295,    15] loss: 0.299\n",
      "Got 395 / 495 with accuracy 79.80\n",
      "[296,     5] loss: 0.420\n",
      "[296,    10] loss: 0.359\n",
      "[296,    15] loss: 0.327\n",
      "Got 403 / 495 with accuracy 81.41\n",
      "[297,     5] loss: 0.298\n",
      "[297,    10] loss: 0.301\n",
      "[297,    15] loss: 0.329\n",
      "Got 407 / 495 with accuracy 82.22\n",
      "[298,     5] loss: 0.359\n",
      "[298,    10] loss: 0.318\n",
      "[298,    15] loss: 0.375\n",
      "Got 387 / 495 with accuracy 78.18\n",
      "[299,     5] loss: 0.320\n",
      "[299,    10] loss: 0.356\n",
      "[299,    15] loss: 0.316\n",
      "Got 380 / 495 with accuracy 76.77\n",
      "[300,     5] loss: 0.372\n",
      "[300,    10] loss: 0.338\n",
      "[300,    15] loss: 0.333\n",
      "Got 393 / 495 with accuracy 79.39\n",
      "Finished Training\n",
      "Saving best model...\n",
      "Saved!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20897<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.04MB of 0.04MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210519_085545-1cp4079e/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210519_085545-1cp4079e/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>0.83618</td></tr><tr><td>Validation Loss</td><td>0.82606</td></tr><tr><td>Train Accuracy</td><td>0.91679</td></tr><tr><td>Validation Accuracy</td><td>0.79394</td></tr><tr><td>Learning Rate</td><td>0.00013</td></tr><tr><td>_step</td><td>299</td></tr><tr><td>_runtime</td><td>36400</td></tr><tr><td>_timestamp</td><td>1621447345</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>▇▃▆█▅▃▃▅▃▅▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>▇▂▅█▄▂▃▄▂▅▁▂▄▁▁▁▁▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>Train Accuracy</td><td>▁▅▃▁▄▅▅▄▆▃▆▆▄▇▇▇▇▆▇▇▆▇▇▇▇▇▇██▇█▇████████</td></tr><tr><td>Validation Accuracy</td><td>▂▆▄▁▅▅▆▄▇▃▇▇▅█▇▇█▇▇▇▇▇▇▇▇▇▇▇██▇▇███▇███▇</td></tr><tr><td>Learning Rate</td><td>██████████████▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">deep-durian-74</strong>: <a href=\"https://wandb.ai/mja2106/laser-trees/runs/1cp4079e\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees/runs/1cp4079e</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wandb.watch(model)\n",
    "best_acc = 0\n",
    "for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    #Training loop============================================\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        depth_images = data['depth_images']\n",
    "        labels = data['labels']\n",
    "\n",
    "        depth_images = depth_images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(depth_images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "                \n",
    "    #Test loop================================================\n",
    "    num_train_correct = 0\n",
    "    num_train_samples = 0\n",
    "    \n",
    "    num_val_correct = 0\n",
    "    num_val_samples = 0\n",
    "    \n",
    "    running_train_loss = 0\n",
    "    running_val_loss = 0\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        #Train set eval==============\n",
    "        for data in train_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_train_correct += (predictions == labels).sum()\n",
    "            num_train_samples += predictions.size(0)\n",
    "            \n",
    "            running_train_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        train_acc = float(num_train_correct)/float(num_train_samples)\n",
    "        train_loss = running_train_loss/len(validation_loader)\n",
    "        \n",
    "        #Test set eval===============\n",
    "        for data in validation_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_val_correct += (predictions == labels).sum()\n",
    "            num_val_samples += predictions.size(0)\n",
    "            \n",
    "            running_val_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        val_acc = float(num_val_correct)/float(num_val_samples)\n",
    "        val_loss = running_val_loss/len(validation_loader)\n",
    "        \n",
    "        print(f'Got {num_val_correct} / {num_val_samples} with accuracy {val_acc*100:.2f}')\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            best_acc = val_acc\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Train Loss\":train_loss,\n",
    "            \"Validation Loss\":val_loss,\n",
    "            \"Train Accuracy\":train_acc,\n",
    "            \"Validation Accuracy\":val_acc,\n",
    "            \"Learning Rate\":optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "print('Saving best model...')\n",
    "torch.save(best_model_state,\n",
    "           '{model_dir}/{fname}.pt'.format(\n",
    "               model_dir=model_dir,\n",
    "               fname=experiment_name+'_best.pt')\n",
    "          )\n",
    "print('Saved!')\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0e1c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8484848484848485"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b5744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
