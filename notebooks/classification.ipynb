{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade4f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "pdir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(pdir)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import utils\n",
    "from simpleview_pytorch import SimpleView\n",
    "\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bbb630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmja2106\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e442c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a148448",
   "metadata": {},
   "source": [
    "### Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a7dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUEFAG     1116\n",
      "PINNIG      581\n",
      "QUEILE      364\n",
      "PINSYL      277\n",
      "PINPIN      140\n",
      "NA            2\n",
      "JUNIPE        2\n",
      "QUERCUS       2\n",
      "DEAD          1\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['DEAD', 'JUNIPE', 'NA', 'PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE', 'QUERCUS']\n",
      "Labels:  tensor([8, 3, 6,  ..., 7, 3, 6])\n",
      "Total count:  2485\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'tree_points.pt'\n",
    "\n",
    "trees_data = torch.load(dataset_name)\n",
    "val_data = torch.load(dataset_name)\n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31719cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">efficient-haze-67</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mja2106/laser-trees\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mja2106/laser-trees/runs/13x651cy\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees/runs/13x651cy</a><br/>\n",
       "                Run data is saved locally in <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210517_142756-13x651cy</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc4fad81f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    \"dataset_type\":type(trees_data),\n",
    "    \"batch_size\":128,\n",
    "    \"validation_split\":.2,\n",
    "    \"shuffle_dataset\":True,\n",
    "    \"random_seed\":0,\n",
    "    \"learning_rate\":0.0005,\n",
    "    \"momentum\":0.9,\n",
    "    \"epochs\":100,\n",
    "    \"loss_fn\":\"cross-entropy\",\n",
    "    \"optimizer\":\"sgd\",\n",
    "    \"jitter\":False,\n",
    "    \"random_rotation\":False,\n",
    "    \"random_scaling\":False,\n",
    "    \"random_translation\":False,\n",
    "    \"voting\":\"None\",\n",
    "    \n",
    "    \"model\":\"SimpleView\",\n",
    "    \n",
    "    \"image_dim\":trees_data.image_dim,\n",
    "    \"camera_fov_deg\":trees_data.camera_fov_deg,\n",
    "    \"f\":trees_data.f,\n",
    "    \"camera_dist\":trees_data.camera_dist,\n",
    "    \"depth_averaging\":\"min\",\n",
    "    \n",
    "    \"transforms\":['rotation','translation','jitter'],\n",
    "    \"min_rotation\":0,\n",
    "    \"max_rotation\":2*np.pi,\n",
    "    \"min_translation\":0,\n",
    "    \"max_translation\":0.5,\n",
    "    \"jitter_std\":3e-4, \n",
    "    \n",
    "    \"species\":[\"QUEFAG\", \"PINNIG\", \"QUEILE\", \"PINSYL\", \"PINPIN\"],\n",
    "    \"data_resolution\":\"2.5cm\"\n",
    "}\n",
    "\n",
    "\n",
    "if params[\"dataset_type\"] == utils.dataset.TreeSpeciesPointDataset: #Change these by hand using point dataset\n",
    "    params[\"image_dim\"] = 256\n",
    "    params[\"camera_fov_deg\"] = 90 \n",
    "    params[\"f\"] = 1\n",
    "    params[\"camera_dist\"] = 1.4\n",
    "    params[\"depth_averaging\"] = \"min\"\n",
    "    params[\"soft_min_k\"] = 50\n",
    "    params[\"num_views\"] = 6\n",
    "    \n",
    "    trees_data.set_params(image_dim = params[\"image_dim\"],\n",
    "                         camera_fov_deg = params[\"camera_fov_deg\"],\n",
    "                         f = params[\"f\"],\n",
    "                         camera_dist = params[\"camera_dist\"],\n",
    "                         soft_min_k = params[\"soft_min_k\"],\n",
    "                         transforms = params[\"transforms\"],\n",
    "                         min_rotation = params[\"min_rotation\"],\n",
    "                         max_rotation = params[\"max_rotation\"],\n",
    "                         min_translation = params[\"min_translation\"],\n",
    "                         max_translation = params[\"max_translation\"],\n",
    "                         jitter_std = params[\"jitter\"]\n",
    "                         )\n",
    "    \n",
    "    val_data.set_params(image_dim = params[\"image_dim\"],\n",
    "                         camera_fov_deg = params[\"camera_fov_deg\"],\n",
    "                         f = params[\"f\"],\n",
    "                         camera_dist = params[\"camera_dist\"],\n",
    "                         soft_min_k = params[\"soft_min_k\"],\n",
    "                         transforms = params[\"transforms\"],\n",
    "                         min_rotation = params[\"min_rotation\"],\n",
    "                         max_rotation = params[\"max_rotation\"],\n",
    "                         min_translation = params[\"min_translation\"],\n",
    "                         max_translation = params[\"max_translation\"],\n",
    "                         jitter_std = params[\"jitter\"]\n",
    "                         )\n",
    "    \n",
    "elif params[\"dataset_type\"] == utils.dataset.TreeSpeciesDataset:\n",
    "    params[\"image_dim\"] = trees_data.image_dim\n",
    "    params[\"camera_fov_deg\"] = trees_data.camera_fov_deg\n",
    "    params[\"f\"] = trees_data.f\n",
    "    params[\"camera_dist\"] = trees_data.camera_dist\n",
    "    params[\"num_views\"] = trees_data.depth_images.shape[1]\n",
    "    params[\"depth_averaging\"] = \"min\"\n",
    "\n",
    "    \n",
    "if trees_data.soft_min_k:\n",
    "    params[\"soft_min_k\"] = trees_data.soft_min_k\n",
    "\n",
    "experiment_name = wandb.util.generate_id()\n",
    "    \n",
    "run = wandb.init(\n",
    "    project='laser-trees',\n",
    "    group=experiment_name,\n",
    "    config=params)    \n",
    "\n",
    "config = wandb.config\n",
    "torch.manual_seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b4be9",
   "metadata": {},
   "source": [
    "#### Remove low-count species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d35f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing: JUNIPE\n",
      "Removing: NA\n",
      "Removing: DEAD\n",
      "Removing: QUERCUS\n",
      "Train Dataset:\n",
      "QUEFAG    1116\n",
      "PINNIG     581\n",
      "QUEILE     364\n",
      "PINSYL     277\n",
      "PINPIN     140\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE']\n",
      "Labels:  tensor([0, 3, 3,  ..., 4, 0, 3])\n",
      "Total count:  2478\n",
      "\n",
      "Validation Dataset (should match):\n",
      "QUEFAG    1116\n",
      "PINNIG     581\n",
      "QUEILE     364\n",
      "PINSYL     277\n",
      "PINPIN     140\n",
      "Name: sp, dtype: int64\n",
      "Species:  ['PINNIG', 'PINPIN', 'PINSYL', 'QUEFAG', 'QUEILE']\n",
      "Labels:  tensor([0, 3, 3,  ..., 4, 0, 3])\n",
      "Total count:  2478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for specie in list(set(trees_data.species) - set(config.species)):\n",
    "    print(\"Removing: {}\".format(specie))\n",
    "    trees_data.remove_species(specie)\n",
    "    val_data.remove_species(specie)\n",
    "\n",
    "print('Train Dataset:')\n",
    "print(trees_data.counts)\n",
    "print('Species: ', trees_data.species)\n",
    "print('Labels: ', trees_data.labels)\n",
    "print('Total count: ', len(trees_data))\n",
    "print()\n",
    "\n",
    "print('Validation Dataset (should match):')\n",
    "print(val_data.counts)\n",
    "print('Species: ', val_data.species)\n",
    "print('Labels: ', val_data.labels)\n",
    "print('Total count: ', len(val_data))\n",
    "print()\n",
    "\n",
    "assert len(val_data) == len(trees_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53b128",
   "metadata": {},
   "source": [
    "#### Train-validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f933abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(trees_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(config.validation_split * dataset_size))\n",
    "\n",
    "if config.shuffle_dataset :\n",
    "    np.random.seed(config.random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb3e4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "\n",
    "val_data.set_params(transforms=['none']) #Turn off transforms for the validation dataset - DON'T GIVE IT AN EMPTY LIST\n",
    "validation_loader = torch.utils.data.DataLoader(trees_data, batch_size=config.batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0087d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to print no. instances of each class\n",
    "if None:\n",
    "    for x in train_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()\n",
    "\n",
    "    for x in validation_loader:\n",
    "        print(torch.unique(x['labels'], return_counts = True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec03e76",
   "metadata": {},
   "source": [
    "### Define model, loss fn, optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9e2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(config.species) == set(trees_data.species)\n",
    "\n",
    "if config.model==\"SimpleView\":\n",
    "    model = SimpleView(\n",
    "        num_views=config.num_views,\n",
    "        num_classes=len(config.species)\n",
    "    )\n",
    "\n",
    "model = model.to(device=device)\n",
    "\n",
    "if config.loss_fn==\"cross-entropy\":\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "if config.loss_fn==\"smooth-loss\":\n",
    "    loss_fn = utils.smooth_loss\n",
    "\n",
    "if config.optimizer==\"sgd\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=config.momentum)\n",
    "elif config.optimizer==\"adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5829a5",
   "metadata": {},
   "source": [
    "### Train & Test Loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c338416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/Work/MRes-Project/laser-trees/utils/utils.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_world_tilde=torch.cat((torch.tensor(cloud), torch.ones(cloud.shape[0],1)), 1).transpose(0,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 3.867\n",
      "[1,    10] loss: 3.560\n",
      "[1,    15] loss: 3.430\n",
      "Got 231 / 495 with accuracy 46.67\n",
      "[2,     5] loss: 3.357\n",
      "[2,    10] loss: 3.383\n",
      "[2,    15] loss: 3.341\n",
      "Got 241 / 495 with accuracy 48.69\n",
      "[3,     5] loss: 3.258\n",
      "[3,    10] loss: 3.253\n",
      "[3,    15] loss: 3.229\n",
      "Got 267 / 495 with accuracy 53.94\n",
      "[4,     5] loss: 3.173\n",
      "[4,    10] loss: 3.175\n",
      "[4,    15] loss: 3.125\n",
      "Got 265 / 495 with accuracy 53.54\n",
      "[5,     5] loss: 3.141\n",
      "[5,    10] loss: 3.040\n",
      "[5,    15] loss: 3.104\n",
      "Got 267 / 495 with accuracy 53.94\n",
      "[6,     5] loss: 3.018\n",
      "[6,    10] loss: 3.156\n",
      "[6,    15] loss: 3.005\n",
      "Got 286 / 495 with accuracy 57.78\n",
      "[7,     5] loss: 2.987\n",
      "[7,    10] loss: 2.947\n",
      "[7,    15] loss: 3.121\n",
      "Got 290 / 495 with accuracy 58.59\n",
      "[8,     5] loss: 2.980\n",
      "[8,    10] loss: 2.976\n",
      "[8,    15] loss: 3.009\n",
      "Got 285 / 495 with accuracy 57.58\n",
      "[9,     5] loss: 3.002\n",
      "[9,    10] loss: 2.991\n",
      "[9,    15] loss: 2.929\n",
      "Got 292 / 495 with accuracy 58.99\n",
      "[10,     5] loss: 2.978\n",
      "[10,    10] loss: 2.928\n",
      "[10,    15] loss: 2.943\n",
      "Got 297 / 495 with accuracy 60.00\n",
      "[11,     5] loss: 2.899\n",
      "[11,    10] loss: 2.909\n",
      "[11,    15] loss: 2.950\n",
      "Got 288 / 495 with accuracy 58.18\n",
      "[12,     5] loss: 2.989\n",
      "[12,    10] loss: 2.831\n",
      "[12,    15] loss: 2.886\n",
      "Got 298 / 495 with accuracy 60.20\n",
      "[13,     5] loss: 2.913\n",
      "[13,    10] loss: 2.848\n",
      "[13,    15] loss: 2.969\n",
      "Got 296 / 495 with accuracy 59.80\n",
      "[14,     5] loss: 2.935\n",
      "[14,    10] loss: 2.874\n",
      "[14,    15] loss: 2.938\n",
      "Got 297 / 495 with accuracy 60.00\n",
      "[15,     5] loss: 2.908\n",
      "[15,    10] loss: 2.920\n",
      "[15,    15] loss: 2.849\n",
      "Got 296 / 495 with accuracy 59.80\n",
      "[16,     5] loss: 2.822\n",
      "[16,    10] loss: 2.914\n",
      "[16,    15] loss: 2.867\n",
      "Got 294 / 495 with accuracy 59.39\n",
      "[17,     5] loss: 2.895\n",
      "[17,    10] loss: 2.894\n",
      "[17,    15] loss: 2.774\n",
      "Got 287 / 495 with accuracy 57.98\n",
      "[18,     5] loss: 2.931\n",
      "[18,    10] loss: 2.719\n",
      "[18,    15] loss: 2.918\n",
      "Got 301 / 495 with accuracy 60.81\n",
      "[19,     5] loss: 2.826\n",
      "[19,    10] loss: 2.870\n",
      "[19,    15] loss: 2.874\n",
      "Got 296 / 495 with accuracy 59.80\n",
      "[20,     5] loss: 2.966\n",
      "[20,    10] loss: 2.774\n",
      "[20,    15] loss: 2.814\n",
      "Got 301 / 495 with accuracy 60.81\n",
      "[21,     5] loss: 2.797\n",
      "[21,    10] loss: 2.874\n",
      "[21,    15] loss: 2.798\n",
      "Got 317 / 495 with accuracy 64.04\n",
      "[22,     5] loss: 2.809\n",
      "[22,    10] loss: 2.882\n",
      "[22,    15] loss: 2.734\n",
      "Got 301 / 495 with accuracy 60.81\n",
      "[23,     5] loss: 2.763\n",
      "[23,    10] loss: 2.828\n",
      "[23,    15] loss: 2.708\n",
      "Got 315 / 495 with accuracy 63.64\n",
      "[24,     5] loss: 2.802\n",
      "[24,    10] loss: 2.701\n",
      "[24,    15] loss: 2.739\n",
      "Got 296 / 495 with accuracy 59.80\n",
      "[25,     5] loss: 2.786\n",
      "[25,    10] loss: 2.678\n",
      "[25,    15] loss: 2.740\n",
      "Got 306 / 495 with accuracy 61.82\n",
      "[26,     5] loss: 2.697\n",
      "[26,    10] loss: 2.764\n",
      "[26,    15] loss: 2.632\n",
      "Got 305 / 495 with accuracy 61.62\n",
      "[27,     5] loss: 2.653\n",
      "[27,    10] loss: 2.724\n",
      "[27,    15] loss: 2.732\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[28,     5] loss: 2.671\n",
      "[28,    10] loss: 2.482\n",
      "[28,    15] loss: 2.744\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[29,     5] loss: 2.595\n",
      "[29,    10] loss: 2.484\n",
      "[29,    15] loss: 2.708\n",
      "Got 313 / 495 with accuracy 63.23\n",
      "[30,     5] loss: 2.561\n",
      "[30,    10] loss: 2.565\n",
      "[30,    15] loss: 2.643\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[31,     5] loss: 2.509\n",
      "[31,    10] loss: 2.637\n",
      "[31,    15] loss: 2.501\n",
      "Got 336 / 495 with accuracy 67.88\n",
      "[32,     5] loss: 2.609\n",
      "[32,    10] loss: 2.520\n",
      "[32,    15] loss: 2.499\n",
      "Got 323 / 495 with accuracy 65.25\n",
      "[33,     5] loss: 2.520\n",
      "[33,    10] loss: 2.356\n",
      "[33,    15] loss: 2.562\n",
      "Got 323 / 495 with accuracy 65.25\n",
      "[34,     5] loss: 2.504\n",
      "[34,    10] loss: 2.418\n",
      "[34,    15] loss: 2.578\n",
      "Got 272 / 495 with accuracy 54.95\n",
      "[35,     5] loss: 2.456\n",
      "[35,    10] loss: 2.498\n",
      "[35,    15] loss: 2.404\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[36,     5] loss: 2.407\n",
      "[36,    10] loss: 2.559\n",
      "[36,    15] loss: 2.413\n",
      "Got 299 / 495 with accuracy 60.40\n",
      "[37,     5] loss: 2.306\n",
      "[37,    10] loss: 2.415\n",
      "[37,    15] loss: 2.606\n",
      "Got 326 / 495 with accuracy 65.86\n",
      "[38,     5] loss: 2.337\n",
      "[38,    10] loss: 2.427\n",
      "[38,    15] loss: 2.325\n",
      "Got 333 / 495 with accuracy 67.27\n",
      "[39,     5] loss: 2.314\n",
      "[39,    10] loss: 2.447\n",
      "[39,    15] loss: 2.374\n",
      "Got 333 / 495 with accuracy 67.27\n",
      "[40,     5] loss: 2.418\n",
      "[40,    10] loss: 2.310\n",
      "[40,    15] loss: 2.348\n",
      "Got 308 / 495 with accuracy 62.22\n",
      "[41,     5] loss: 2.380\n",
      "[41,    10] loss: 2.274\n",
      "[41,    15] loss: 2.397\n",
      "Got 343 / 495 with accuracy 69.29\n",
      "[42,     5] loss: 2.341\n",
      "[42,    10] loss: 2.362\n",
      "[42,    15] loss: 2.284\n",
      "Got 312 / 495 with accuracy 63.03\n",
      "[43,     5] loss: 2.518\n",
      "[43,    10] loss: 2.256\n",
      "[43,    15] loss: 2.272\n",
      "Got 335 / 495 with accuracy 67.68\n",
      "[44,     5] loss: 2.362\n",
      "[44,    10] loss: 2.272\n",
      "[44,    15] loss: 2.261\n",
      "Got 286 / 495 with accuracy 57.78\n",
      "[45,     5] loss: 2.254\n",
      "[45,    10] loss: 2.239\n",
      "[45,    15] loss: 2.428\n",
      "Got 310 / 495 with accuracy 62.63\n",
      "[46,     5] loss: 2.312\n",
      "[46,    10] loss: 2.251\n",
      "[46,    15] loss: 2.479\n",
      "Got 329 / 495 with accuracy 66.46\n",
      "[47,     5] loss: 2.304\n",
      "[47,    10] loss: 2.269\n",
      "[47,    15] loss: 2.256\n",
      "Got 311 / 495 with accuracy 62.83\n",
      "[48,     5] loss: 2.425\n",
      "[48,    10] loss: 2.383\n",
      "[48,    15] loss: 2.180\n",
      "Got 335 / 495 with accuracy 67.68\n",
      "[49,     5] loss: 2.252\n",
      "[49,    10] loss: 2.333\n",
      "[49,    15] loss: 2.275\n",
      "Got 319 / 495 with accuracy 64.44\n",
      "[50,     5] loss: 2.257\n",
      "[50,    10] loss: 2.211\n",
      "[50,    15] loss: 2.279\n",
      "Got 326 / 495 with accuracy 65.86\n",
      "[51,     5] loss: 2.185\n",
      "[51,    10] loss: 2.312\n",
      "[51,    15] loss: 2.254\n",
      "Got 341 / 495 with accuracy 68.89\n",
      "[52,     5] loss: 2.271\n",
      "[52,    10] loss: 2.234\n",
      "[52,    15] loss: 2.220\n",
      "Got 287 / 495 with accuracy 57.98\n",
      "[53,     5] loss: 2.356\n",
      "[53,    10] loss: 2.357\n",
      "[53,    15] loss: 2.093\n",
      "Got 242 / 495 with accuracy 48.89\n",
      "[54,     5] loss: 2.229\n",
      "[54,    10] loss: 2.254\n",
      "[54,    15] loss: 2.355\n",
      "Got 336 / 495 with accuracy 67.88\n",
      "[55,     5] loss: 2.210\n",
      "[55,    10] loss: 2.192\n",
      "[55,    15] loss: 2.300\n",
      "Got 336 / 495 with accuracy 67.88\n",
      "[56,     5] loss: 2.102\n",
      "[56,    10] loss: 2.222\n",
      "[56,    15] loss: 2.315\n",
      "Got 256 / 495 with accuracy 51.72\n",
      "[57,     5] loss: 2.303\n",
      "[57,    10] loss: 2.128\n",
      "[57,    15] loss: 2.282\n",
      "Got 293 / 495 with accuracy 59.19\n",
      "[58,     5] loss: 2.216\n",
      "[58,    10] loss: 2.191\n",
      "[58,    15] loss: 2.274\n",
      "Got 330 / 495 with accuracy 66.67\n",
      "[59,     5] loss: 2.243\n",
      "[59,    10] loss: 2.238\n",
      "[59,    15] loss: 2.182\n",
      "Got 318 / 495 with accuracy 64.24\n",
      "[60,     5] loss: 2.238\n",
      "[60,    10] loss: 2.286\n",
      "[60,    15] loss: 2.153\n",
      "Got 344 / 495 with accuracy 69.49\n",
      "[61,     5] loss: 2.186\n",
      "[61,    10] loss: 2.275\n",
      "[61,    15] loss: 2.131\n",
      "Got 323 / 495 with accuracy 65.25\n",
      "[62,     5] loss: 2.062\n",
      "[62,    10] loss: 2.238\n",
      "[62,    15] loss: 2.323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4d8c64399b0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#Train set eval==============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mdepth_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth_images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/laser-trees/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/laser-trees/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/laser-trees/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/laser-trees/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/MRes-Project/laser-trees/utils/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mdepth_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_depth_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mdepth_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_depth_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/MRes-Project/laser-trees/utils/dataset.py\u001b[0m in \u001b[0;36mget_depth_image\u001b[0;34m(self, i, transforms)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'jitter'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             points = self.jitter(points,\n\u001b[0m\u001b[1;32m    203\u001b[0m                                  jitter_std = self.jitter_std)\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/MRes-Project/laser-trees/utils/dataset.py\u001b[0m in \u001b[0;36mjitter\u001b[0;34m(self, point_cloud, jitter_std)\u001b[0m\n\u001b[1;32m    305\u001b[0m                jitter_std = 3e-4):\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mcloud_jitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjitter_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpoint_cloud\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcloud_jitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#wandb.watch(model)\n",
    "for epoch in range(config.epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "    #Training loop============================================\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        depth_images = data['depth_images']\n",
    "        labels = data['labels']\n",
    "\n",
    "        depth_images = depth_images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(depth_images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "                \n",
    "    #Test loop================================================\n",
    "    num_train_correct = 0\n",
    "    num_train_samples = 0\n",
    "    \n",
    "    num_val_correct = 0\n",
    "    num_val_samples = 0\n",
    "    \n",
    "    running_train_loss = 0\n",
    "    running_val_loss = 0\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        #Train set eval==============\n",
    "        for data in train_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_train_correct += (predictions == labels).sum()\n",
    "            num_train_samples += predictions.size(0)\n",
    "            \n",
    "            running_train_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        train_acc = float(num_train_correct)/float(num_train_samples)\n",
    "        train_loss = running_train_loss/len(validation_loader)\n",
    "        \n",
    "        #Test set eval===============\n",
    "        for data in validation_loader:\n",
    "            depth_images = data['depth_images']\n",
    "            labels = data['labels']\n",
    "\n",
    "            depth_images = depth_images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            scores = model(depth_images)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_val_correct += (predictions == labels).sum()\n",
    "            num_val_samples += predictions.size(0)\n",
    "            \n",
    "            running_val_loss += loss_fn(scores, labels)\n",
    "        \n",
    "        val_acc = float(num_val_correct)/float(num_val_samples)\n",
    "        val_loss = running_val_loss/len(validation_loader)\n",
    "        \n",
    "        print(f'Got {num_val_correct} / {num_val_samples} with accuracy {val_acc*100:.2f}')\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Train Loss\":train_loss,\n",
    "            \"Validation Loss\":val_loss,\n",
    "            \"Train Accuracy\":train_acc,\n",
    "            \"Validation Accuracy\":val_acc\n",
    "            })\n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb2a9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9512<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.05MB of 0.05MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210517_142756-13x651cy/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/matt/Work/MRes-Project/laser-trees/notebooks/wandb/run-20210517_142756-13x651cy/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>3.78944</td></tr><tr><td>Validation Loss</td><td>0.92112</td></tr><tr><td>Train Accuracy</td><td>0.6349</td></tr><tr><td>Validation Accuracy</td><td>0.65253</td></tr><tr><td>_step</td><td>60</td></tr><tr><td>_runtime</td><td>7159</td></tr><tr><td>_timestamp</td><td>1621265235</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>█▇▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▂▅▄▂▁▂▂▄▂▃▁▂▁▆▁▅▅▂▂</td></tr><tr><td>Validation Loss</td><td>█▇▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▅▃▁▁▁▂▄▂▃▁▂▁▆▁▅▄▂▁</td></tr><tr><td>Train Accuracy</td><td>▁▁▃▃▅▄▅▅▅▅▅▅▅▆▆▅▆▆▆▆▇▇▄▄▇█▇▆▄▇▅█▇█▂█▃▅▇▇</td></tr><tr><td>Validation Accuracy</td><td>▁▂▃▃▅▄▅▅▅▅▅▅▅▆▅▅▆▆▆▇█▇▄▅▇▇█▆▄▆▆▇▇█▂█▃▅▆▇</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">efficient-haze-67</strong>: <a href=\"https://wandb.ai/mja2106/laser-trees/runs/13x651cy\" target=\"_blank\">https://wandb.ai/mja2106/laser-trees/runs/13x651cy</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e26fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
